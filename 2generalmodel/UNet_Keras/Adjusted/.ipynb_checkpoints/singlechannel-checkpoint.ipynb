{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29603,
     "status": "ok",
     "timestamp": 1631032550584,
     "user": {
      "displayName": "Sijin Li",
      "photoUrl": "",
      "userId": "03828829591723007437"
     },
     "user_tz": -480
    },
    "id": "ndouDEi6rJ4_",
    "outputId": "15d48204-b434-4ef7-a2d0-9101ca4eddaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1363,
     "status": "ok",
     "timestamp": 1631032551940,
     "user": {
      "displayName": "Sijin Li",
      "photoUrl": "",
      "userId": "03828829591723007437"
     },
     "user_tz": -480
    },
    "id": "VrJ4ui0N7cIC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1404,
     "status": "ok",
     "timestamp": 1631032553340,
     "user": {
      "displayName": "Sijin Li",
      "photoUrl": "",
      "userId": "03828829591723007437"
     },
     "user_tz": -480
    },
    "id": "aBSSGOSHrSXi"
   },
   "outputs": [],
   "source": [
    "##迭代器################################\n",
    "\n",
    "from tensorflow.compat.v1 import keras\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np \n",
    "import os\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import skimage.transform as trans\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from tensorflow.compat.v1.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, concatenate, UpSampling2D\n",
    "\n",
    "IMG_SIZE = 224\n",
    "IMG_BANDS = 3\n",
    "# =============================================================================\n",
    "# ##这一步获取了原图和mask\n",
    "def trainGenerator(batch_size,train_path,train_path2,image_folder,mask_folder,aug_dict,\n",
    "                    flag_multi_class, num_class ,image_color_mode = 'rgb',#\"grayscale\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",save_to_dir = None,target_size = (IMG_SIZE,IMG_SIZE),seed = 1):\n",
    "\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    print(image_generator)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path2,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    new_mask = np.ones([batch_size,224,224,2])\n",
    "    for (img,mask) in train_generator:\n",
    "        #img = img / 255\n",
    "        #print('mask',mask.shape)\n",
    "        img,mask = adjustData(img,mask,\n",
    "                           flag_multi_class,   ##参数（flag_multi_class）用来开启多分类\n",
    "                           num_class,new_mask)##这里对图像进行了处理，用以减小计算量（函数定义在下面）\n",
    "        yield (img,mask)##用生成器进行迭代数据，可以传入model.fit_generator（）这个函数进行训练\n",
    "        new_mask = np.ones([batch_size,224,224,2])\n",
    "#    return img, mask\n",
    "\n",
    "def adjustData(img,mask,flag_multi_class,num_class,new_mask):\n",
    "    if (flag_multi_class):#如果多分类，在mask添加多层，每层对应一个类别\n",
    "        img = img / 255\n",
    "    return (img,mask)\n",
    "\n",
    "\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1631032553341,
     "user": {
      "displayName": "Sijin Li",
      "photoUrl": "",
      "userId": "03828829591723007437"
     },
     "user_tz": -480
    },
    "id": "OME4JFkGrSeS"
   },
   "outputs": [],
   "source": [
    "img_path = '/content/drive/MyDrive/2105Dinghu/data/v2/train/'\n",
    "mask_path = '/content/drive/MyDrive/2105Dinghu/data/v2/train/'\n",
    "img_files = 'image_crop'\n",
    "mask_files = 'mask_crop'\n",
    "num_class = {'0','1'}\n",
    "\n",
    "\n",
    "data_gen_args = dict(rotation_range=0.2,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "myGene = trainGenerator(2,img_path,         #图像路径\n",
    "                            mask_path,    #mask路径\n",
    "                            img_files,      #mask文件夹名称\n",
    "                            mask_files,     #图像文件夹名称（不懂的话看此函数定义和keras文档）\n",
    "                            data_gen_args,True,#\n",
    "                            num_class,target_size=(IMG_SIZE,IMG_SIZE))\n",
    "\n",
    "img_valid_path = '/content/drive/MyDrive/2105Dinghu/data/v2/valid/'\n",
    "mask_valid_path = '/content/drive/MyDrive/2105Dinghu/data/v2/valid/'\n",
    "valid_img_files = 'image_crop'\n",
    "valid_mask_files = 'mask_crop'\n",
    "valid_data = trainGenerator(2,img_valid_path,         #图像路径\n",
    "                            mask_valid_path,    #mask路径\n",
    "                            valid_img_files,      #mask文件夹名称\n",
    "                            valid_mask_files,     #图像文件夹名称（不懂的话看此函数定义和keras文档）\n",
    "                            data_gen_args,True,#\n",
    "                            num_class,target_size=(IMG_SIZE,IMG_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1274,
     "status": "ok",
     "timestamp": 1630398273481,
     "user": {
      "displayName": "sijin li",
      "photoUrl": "",
      "userId": "04019906230177487561"
     },
     "user_tz": -480
    },
    "id": "h4B9OlIqrSi_",
    "outputId": "2c484987-fa98-465d-c761-bde8808b8bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 224, 224, 64) 1792        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 224, 224, 64) 36928       conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 112, 112, 64) 0           conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 112, 112, 128 73856       max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 112, 112, 128 147584      conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 56, 56, 128)  0           conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 56, 56, 256)  295168      max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 56, 56, 256)  590080      conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 28, 28, 256)  0           conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 28, 28, 512)  1180160     max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 28, 28, 512)  2359808     conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 28, 28, 512)  2359808     conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 14, 14, 512)  0           conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_21 (UpSampling2D) (None, 28, 28, 512)  0           max_pooling2d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 28, 28, 512)  1049088     up_sampling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 28, 28, 1024) 0           conv2d_120[0][0]                 \n",
      "                                                                 conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 28, 28, 512)  4719104     concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 28, 28, 512)  2359808     conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_22 (UpSampling2D) (None, 56, 56, 512)  0           conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 56, 56, 256)  524544      up_sampling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 56, 56, 512)  0           conv2d_117[0][0]                 \n",
      "                                                                 conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 56, 56, 256)  1179904     concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 56, 56, 256)  590080      conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_23 (UpSampling2D) (None, 112, 112, 256 0           conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 112, 112, 128 131200      up_sampling2d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 112, 112, 256 0           conv2d_115[0][0]                 \n",
      "                                                                 conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 112, 112, 128 295040      concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 112, 112, 128 147584      conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_24 (UpSampling2D) (None, 224, 224, 128 0           conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 224, 224, 64) 32832       up_sampling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 224, 224, 128 0           conv2d_113[0][0]                 \n",
      "                                                                 conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 224, 224, 64) 73792       concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 224, 224, 64) 36928       conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 224, 224, 8)  4616        conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "result (Conv2D)                 (None, 224, 224, 1)  9           conv2d_139[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 18,189,713\n",
      "Trainable params: 18,189,713\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "# with max pooling\n",
    "TARGET_SIZE = 224\n",
    "BANDS = 3\n",
    "def unet(pretrained_weights = None,input_size = (TARGET_SIZE, TARGET_SIZE, BANDS)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    conv4 = Conv2D(512, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.7)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    #===================================================#\n",
    "\n",
    "    conv5 = Conv2D(1024, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    conv5 = Conv2D(1024, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    pool5 = MaxPooling2D(pool_size=(2, 2))(drop5)\n",
    "    \n",
    "    #conv5_2 = Conv2D(1024, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool5)\n",
    "    #conv5_2 = Conv2D(1024, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5_2)\n",
    "    #drop5_2 = Dropout(0.5)(conv5_2)\n",
    "    #pool5_2 = MaxPooling2D(pool_size=(2, 2))(drop5_2)\n",
    "\n",
    "    #up6_2 = Conv2D(1024, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(pool5_2))\n",
    "    #merge6_2 = concatenate([drop5_2,up6_2],)\n",
    "    #conv6_2 = Conv2D(1024, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6_2)\n",
    "    #conv6_2 = Conv2D(1024, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6_2)\n",
    "    \n",
    "    up6 = Conv2D(1024, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(pool5))\n",
    "    merge6 = concatenate([drop5,up6],)\n",
    "    conv6 = Conv2D(1024, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(1024, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "     #===================================================#\n",
    "    up7 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(pool4))\n",
    "    merge7 = concatenate([conv4,up7],)\n",
    "    conv7 = Conv2D(512, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(512, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv3,up8], axis = 3)\n",
    "    conv8 = Conv2D(256, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(256, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv2,up9], axis = 3)\n",
    "    conv9 = Conv2D(128, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(128, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "\n",
    "    up10 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv9))\n",
    "    merge10 = concatenate([conv1,up10], axis = 3)\n",
    "    conv10 = Conv2D(64, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge10)\n",
    "    conv10 = Conv2D(64, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv10)\n",
    "    \n",
    "    #up11 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv10))\n",
    "    #merge11 = concatenate([conv1,up11], axis = 3)\n",
    "    #conv11 = Conv2D(64, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge11)\n",
    "    #conv11 = Conv2D(64, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv11)\n",
    "    conv11 = Conv2D(8, kernel_size=(3,3), strides=1, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv10)\n",
    "    conv12 = Conv2D(1, 1, activation = 'sigmoid', name = 'result')(conv11) ##注意output层\n",
    "                                                         \n",
    "    model = Model(inputs = inputs, outputs = conv12)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['accuracy']) #Adam(lr = 1e-5)\n",
    "    \n",
    "    #model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "    \tmodel.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n",
    "# =============================================================================\n",
    "model = unet()\n",
    "#model_checkpoint = ModelCheckpoint('saver\\module_1.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNqweIMK7Cle"
   },
   "outputs": [],
   "source": [
    "#存储loss和acc\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        #plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            #plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2321886,
     "status": "ok",
     "timestamp": 1630397137205,
     "user": {
      "displayName": "sijin li",
      "photoUrl": "",
      "userId": "04019906230177487561"
     },
     "user_tz": -480
    },
    "id": "3QZ9SzdVrdGN",
    "outputId": "2a9c715a-fe08-4d63-ce0e-995fd9f7ae11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:1228: UserWarning: `model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 images belonging to 1 classes.\n",
      "<keras.preprocessing.image.DirectoryIterator object at 0x7f254ec3bb10>\n",
      "Found 39 images belonging to 1 classes.\n",
      "Epoch 1/500\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.6296 - accuracy: 0.7104Found 9 images belonging to 1 classes.\n",
      "<keras.preprocessing.image.DirectoryIterator object at 0x7f254b86cb90>\n",
      "Found 9 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "10/10 [==============================] - 15s 1s/step - batch: 4.5000 - size: 2.0000 - loss: 0.6296 - accuracy: 0.7104 - val_loss: 0.5684 - val_accuracy: 0.7468\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - 6s 601ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.5705 - accuracy: 0.7194 - val_loss: 0.4909 - val_accuracy: 0.7928\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.5014 - accuracy: 0.7707 - val_loss: 0.4258 - val_accuracy: 0.7975\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - 4s 456ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.4265 - accuracy: 0.7944 - val_loss: 0.3954 - val_accuracy: 0.7888\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - 5s 472ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.4230 - accuracy: 0.8053 - val_loss: 0.3704 - val_accuracy: 0.8029\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - 4s 458ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.4024 - accuracy: 0.7786 - val_loss: 0.3878 - val_accuracy: 0.8152\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - 5s 472ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.4078 - accuracy: 0.8154 - val_loss: 0.3601 - val_accuracy: 0.8210\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - 4s 457ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.4027 - accuracy: 0.7889 - val_loss: 0.3975 - val_accuracy: 0.7967\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - 5s 473ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3790 - accuracy: 0.7915 - val_loss: 0.3562 - val_accuracy: 0.7990\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - 4s 458ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.3640 - accuracy: 0.8114 - val_loss: 0.3438 - val_accuracy: 0.8055\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - 5s 476ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3143 - accuracy: 0.8479 - val_loss: 0.3041 - val_accuracy: 0.8318\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - 4s 460ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.3230 - accuracy: 0.8467 - val_loss: 0.2937 - val_accuracy: 0.8663\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - 5s 478ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3021 - accuracy: 0.8463 - val_loss: 0.2470 - val_accuracy: 0.8742\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.3228 - accuracy: 0.8314 - val_loss: 0.2481 - val_accuracy: 0.8942\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - 5s 477ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2499 - accuracy: 0.8781 - val_loss: 0.2824 - val_accuracy: 0.8649\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.3368 - accuracy: 0.8357 - val_loss: 0.2487 - val_accuracy: 0.8843\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - 5s 478ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2653 - accuracy: 0.8778 - val_loss: 0.2331 - val_accuracy: 0.8771\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2679 - accuracy: 0.8634 - val_loss: 0.2203 - val_accuracy: 0.8928\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - 5s 477ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2578 - accuracy: 0.8735 - val_loss: 0.2689 - val_accuracy: 0.8748\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - 5s 463ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2738 - accuracy: 0.8731 - val_loss: 0.2265 - val_accuracy: 0.8916\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2776 - accuracy: 0.8703 - val_loss: 0.2294 - val_accuracy: 0.8876\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1974 - accuracy: 0.9017 - val_loss: 0.2331 - val_accuracy: 0.8887\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2589 - accuracy: 0.8812 - val_loss: 0.2217 - val_accuracy: 0.8988\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1977 - accuracy: 0.9123 - val_loss: 0.2064 - val_accuracy: 0.9106\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - 5s 478ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2436 - accuracy: 0.8894 - val_loss: 0.2036 - val_accuracy: 0.9093\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1966 - accuracy: 0.9119 - val_loss: 0.2431 - val_accuracy: 0.8929\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1997 - accuracy: 0.9097 - val_loss: 0.2322 - val_accuracy: 0.8980\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.3217 - accuracy: 0.8490 - val_loss: 0.3266 - val_accuracy: 0.8566\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3002 - accuracy: 0.8510 - val_loss: 0.2678 - val_accuracy: 0.8751\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2157 - accuracy: 0.8964 - val_loss: 0.2489 - val_accuracy: 0.8815\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2224 - accuracy: 0.8990 - val_loss: 0.2452 - val_accuracy: 0.8930\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2212 - accuracy: 0.8905 - val_loss: 0.2016 - val_accuracy: 0.9066\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1797 - accuracy: 0.9133 - val_loss: 0.2093 - val_accuracy: 0.9057\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2395 - accuracy: 0.8957 - val_loss: 0.2066 - val_accuracy: 0.9108\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - 5s 477ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2124 - accuracy: 0.9018 - val_loss: 0.2044 - val_accuracy: 0.9108\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - 4s 463ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1987 - accuracy: 0.9173 - val_loss: 0.2458 - val_accuracy: 0.8935\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2208 - accuracy: 0.8964 - val_loss: 0.2477 - val_accuracy: 0.8858\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1962 - accuracy: 0.9133 - val_loss: 0.2044 - val_accuracy: 0.9169\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1640 - accuracy: 0.9312 - val_loss: 0.2241 - val_accuracy: 0.9037\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - 4s 462ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1869 - accuracy: 0.9148 - val_loss: 0.2053 - val_accuracy: 0.9131\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1986 - accuracy: 0.9121 - val_loss: 0.2023 - val_accuracy: 0.9114\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1659 - accuracy: 0.9267 - val_loss: 0.1802 - val_accuracy: 0.9210\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1517 - accuracy: 0.9314 - val_loss: 0.1765 - val_accuracy: 0.9175\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1794 - accuracy: 0.9205 - val_loss: 0.2006 - val_accuracy: 0.9211\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1528 - accuracy: 0.9318 - val_loss: 0.1641 - val_accuracy: 0.9231\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1697 - accuracy: 0.9260 - val_loss: 0.2072 - val_accuracy: 0.9132\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - 5s 478ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1480 - accuracy: 0.9319 - val_loss: 0.1884 - val_accuracy: 0.9194\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1831 - accuracy: 0.9191 - val_loss: 0.1984 - val_accuracy: 0.9166\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1499 - accuracy: 0.9347 - val_loss: 0.1961 - val_accuracy: 0.9159\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1508 - accuracy: 0.9309 - val_loss: 0.1747 - val_accuracy: 0.9216\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1631 - accuracy: 0.9255 - val_loss: 0.1747 - val_accuracy: 0.9201\n",
      "Epoch 52/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1686 - accuracy: 0.9233 - val_loss: 0.2002 - val_accuracy: 0.9152\n",
      "Epoch 53/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1657 - accuracy: 0.9262 - val_loss: 0.2229 - val_accuracy: 0.9031\n",
      "Epoch 54/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1589 - accuracy: 0.9290 - val_loss: 0.1759 - val_accuracy: 0.9251\n",
      "Epoch 55/500\n",
      "10/10 [==============================] - 5s 477ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1423 - accuracy: 0.9360 - val_loss: 0.1710 - val_accuracy: 0.9259\n",
      "Epoch 56/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1464 - accuracy: 0.9349 - val_loss: 0.1893 - val_accuracy: 0.9156\n",
      "Epoch 57/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1895 - accuracy: 0.9118 - val_loss: 0.1654 - val_accuracy: 0.9231\n",
      "Epoch 58/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1522 - accuracy: 0.9330 - val_loss: 0.2883 - val_accuracy: 0.8892\n",
      "Epoch 59/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1650 - accuracy: 0.9239 - val_loss: 0.1944 - val_accuracy: 0.9083\n",
      "Epoch 60/500\n",
      "10/10 [==============================] - 5s 462ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1763 - accuracy: 0.9208 - val_loss: 0.1576 - val_accuracy: 0.9238\n",
      "Epoch 61/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1517 - accuracy: 0.9331 - val_loss: 0.1791 - val_accuracy: 0.9243\n",
      "Epoch 62/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1396 - accuracy: 0.9357 - val_loss: 0.1811 - val_accuracy: 0.9225\n",
      "Epoch 63/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1248 - accuracy: 0.9436 - val_loss: 0.1559 - val_accuracy: 0.9354\n",
      "Epoch 64/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1435 - accuracy: 0.9303 - val_loss: 0.1941 - val_accuracy: 0.9210\n",
      "Epoch 65/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1316 - accuracy: 0.9395 - val_loss: 0.1735 - val_accuracy: 0.9224\n",
      "Epoch 66/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1574 - accuracy: 0.9345 - val_loss: 0.1576 - val_accuracy: 0.9298\n",
      "Epoch 67/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1106 - accuracy: 0.9505 - val_loss: 0.1662 - val_accuracy: 0.9250\n",
      "Epoch 68/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1528 - accuracy: 0.9306 - val_loss: 0.1719 - val_accuracy: 0.9235\n",
      "Epoch 69/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1251 - accuracy: 0.9423 - val_loss: 0.1676 - val_accuracy: 0.9243\n",
      "Epoch 70/500\n",
      "10/10 [==============================] - 5s 463ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1294 - accuracy: 0.9399 - val_loss: 0.1642 - val_accuracy: 0.9294\n",
      "Epoch 71/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1349 - accuracy: 0.9387 - val_loss: 0.1769 - val_accuracy: 0.9289\n",
      "Epoch 72/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1258 - accuracy: 0.9427 - val_loss: 0.1772 - val_accuracy: 0.9274\n",
      "Epoch 73/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1065 - accuracy: 0.9518 - val_loss: 0.1603 - val_accuracy: 0.9325\n",
      "Epoch 74/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1403 - accuracy: 0.9345 - val_loss: 0.1777 - val_accuracy: 0.9269\n",
      "Epoch 75/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1383 - accuracy: 0.9348 - val_loss: 0.1625 - val_accuracy: 0.9235\n",
      "Epoch 76/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1117 - accuracy: 0.9479 - val_loss: 0.1670 - val_accuracy: 0.9296\n",
      "Epoch 77/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1239 - accuracy: 0.9438 - val_loss: 0.1842 - val_accuracy: 0.9207\n",
      "Epoch 78/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1287 - accuracy: 0.9415 - val_loss: 0.1442 - val_accuracy: 0.9339\n",
      "Epoch 79/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1069 - accuracy: 0.9513 - val_loss: 0.1451 - val_accuracy: 0.9340\n",
      "Epoch 80/500\n",
      "10/10 [==============================] - 5s 463ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1330 - accuracy: 0.9353 - val_loss: 0.1602 - val_accuracy: 0.9318\n",
      "Epoch 81/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1290 - accuracy: 0.9394 - val_loss: 0.1776 - val_accuracy: 0.9316\n",
      "Epoch 82/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1100 - accuracy: 0.9497 - val_loss: 0.1730 - val_accuracy: 0.9272\n",
      "Epoch 83/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1116 - accuracy: 0.9468 - val_loss: 0.1611 - val_accuracy: 0.9307\n",
      "Epoch 84/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1140 - accuracy: 0.9424 - val_loss: 0.1995 - val_accuracy: 0.9199\n",
      "Epoch 85/500\n",
      "10/10 [==============================] - 5s 478ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1346 - accuracy: 0.9375 - val_loss: 0.1854 - val_accuracy: 0.9203\n",
      "Epoch 86/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1106 - accuracy: 0.9493 - val_loss: 0.1638 - val_accuracy: 0.9320\n",
      "Epoch 87/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1469 - accuracy: 0.9305 - val_loss: 0.2054 - val_accuracy: 0.9227\n",
      "Epoch 88/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1224 - accuracy: 0.9447 - val_loss: 0.1313 - val_accuracy: 0.9417\n",
      "Epoch 89/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1020 - accuracy: 0.9525 - val_loss: 0.1577 - val_accuracy: 0.9392\n",
      "Epoch 90/500\n",
      "10/10 [==============================] - 5s 463ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1214 - accuracy: 0.9405 - val_loss: 0.1648 - val_accuracy: 0.9289\n",
      "Epoch 91/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1087 - accuracy: 0.9500 - val_loss: 0.1498 - val_accuracy: 0.9324\n",
      "Epoch 92/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1139 - accuracy: 0.9458 - val_loss: 0.2031 - val_accuracy: 0.9216\n",
      "Epoch 93/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0977 - accuracy: 0.9532 - val_loss: 0.1523 - val_accuracy: 0.9318\n",
      "Epoch 94/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1313 - accuracy: 0.9380 - val_loss: 0.1866 - val_accuracy: 0.9279\n",
      "Epoch 95/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1233 - accuracy: 0.9425 - val_loss: 0.1597 - val_accuracy: 0.9355\n",
      "Epoch 96/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1194 - accuracy: 0.9449 - val_loss: 0.1554 - val_accuracy: 0.9331\n",
      "Epoch 97/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1157 - accuracy: 0.9456 - val_loss: 0.1653 - val_accuracy: 0.9318\n",
      "Epoch 98/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1088 - accuracy: 0.9509 - val_loss: 0.1846 - val_accuracy: 0.9259\n",
      "Epoch 99/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1096 - accuracy: 0.9478 - val_loss: 0.1592 - val_accuracy: 0.9306\n",
      "Epoch 100/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1097 - accuracy: 0.9486 - val_loss: 0.1933 - val_accuracy: 0.9202\n",
      "Epoch 101/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1246 - accuracy: 0.9413 - val_loss: 0.1545 - val_accuracy: 0.9302\n",
      "Epoch 102/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1056 - accuracy: 0.9470 - val_loss: 0.1724 - val_accuracy: 0.9223\n",
      "Epoch 103/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1070 - accuracy: 0.9497 - val_loss: 0.1399 - val_accuracy: 0.9387\n",
      "Epoch 104/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1207 - accuracy: 0.9451 - val_loss: 0.1668 - val_accuracy: 0.9390\n",
      "Epoch 105/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1214 - accuracy: 0.9420 - val_loss: 0.1671 - val_accuracy: 0.9283\n",
      "Epoch 106/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0928 - accuracy: 0.9575 - val_loss: 0.1499 - val_accuracy: 0.9342\n",
      "Epoch 107/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1004 - accuracy: 0.9508 - val_loss: 0.1686 - val_accuracy: 0.9355\n",
      "Epoch 108/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1045 - accuracy: 0.9521 - val_loss: 0.2041 - val_accuracy: 0.9183\n",
      "Epoch 109/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1058 - accuracy: 0.9467 - val_loss: 0.1625 - val_accuracy: 0.9298\n",
      "Epoch 110/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1046 - accuracy: 0.9514 - val_loss: 0.1653 - val_accuracy: 0.9361\n",
      "Epoch 111/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0951 - accuracy: 0.9532 - val_loss: 0.1598 - val_accuracy: 0.9335\n",
      "Epoch 112/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1192 - accuracy: 0.9449 - val_loss: 0.1550 - val_accuracy: 0.9356\n",
      "Epoch 113/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1158 - accuracy: 0.9444 - val_loss: 0.2088 - val_accuracy: 0.9254\n",
      "Epoch 114/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1253 - accuracy: 0.9494 - val_loss: 0.1398 - val_accuracy: 0.9346\n",
      "Epoch 115/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1104 - accuracy: 0.9487 - val_loss: 0.1565 - val_accuracy: 0.9323\n",
      "Epoch 116/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1100 - accuracy: 0.9462 - val_loss: 0.1795 - val_accuracy: 0.9302\n",
      "Epoch 117/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0982 - accuracy: 0.9530 - val_loss: 0.1959 - val_accuracy: 0.9232\n",
      "Epoch 118/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1060 - accuracy: 0.9475 - val_loss: 0.1681 - val_accuracy: 0.9289\n",
      "Epoch 119/500\n",
      "10/10 [==============================] - 5s 478ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0815 - accuracy: 0.9600 - val_loss: 0.1785 - val_accuracy: 0.9335\n",
      "Epoch 120/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1099 - accuracy: 0.9457 - val_loss: 0.1342 - val_accuracy: 0.9382\n",
      "Epoch 121/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1119 - accuracy: 0.9462 - val_loss: 0.2483 - val_accuracy: 0.9197\n",
      "Epoch 122/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1527 - accuracy: 0.9359 - val_loss: 0.1614 - val_accuracy: 0.9281\n",
      "Epoch 123/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1493 - accuracy: 0.9283 - val_loss: 0.1805 - val_accuracy: 0.9246\n",
      "Epoch 124/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1327 - accuracy: 0.9356 - val_loss: 0.1614 - val_accuracy: 0.9369\n",
      "Epoch 125/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1169 - accuracy: 0.9461 - val_loss: 0.1402 - val_accuracy: 0.9380\n",
      "Epoch 126/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1007 - accuracy: 0.9519 - val_loss: 0.1537 - val_accuracy: 0.9355\n",
      "Epoch 127/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1099 - accuracy: 0.9478 - val_loss: 0.1533 - val_accuracy: 0.9380\n",
      "Epoch 128/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0987 - accuracy: 0.9535 - val_loss: 0.1574 - val_accuracy: 0.9356\n",
      "Epoch 129/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1039 - accuracy: 0.9506 - val_loss: 0.1275 - val_accuracy: 0.9482\n",
      "Epoch 130/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1089 - accuracy: 0.9493 - val_loss: 0.1569 - val_accuracy: 0.9351\n",
      "Epoch 131/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1051 - accuracy: 0.9484 - val_loss: 0.1503 - val_accuracy: 0.9326\n",
      "Epoch 132/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1046 - accuracy: 0.9519 - val_loss: 0.1279 - val_accuracy: 0.9417\n",
      "Epoch 133/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1057 - accuracy: 0.9474 - val_loss: 0.1922 - val_accuracy: 0.9332\n",
      "Epoch 134/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0831 - accuracy: 0.9583 - val_loss: 0.1648 - val_accuracy: 0.9357\n",
      "Epoch 135/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0914 - accuracy: 0.9539 - val_loss: 0.1718 - val_accuracy: 0.9314\n",
      "Epoch 136/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1003 - accuracy: 0.9525 - val_loss: 0.1563 - val_accuracy: 0.9342\n",
      "Epoch 137/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0854 - accuracy: 0.9600 - val_loss: 0.1418 - val_accuracy: 0.9386\n",
      "Epoch 138/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1002 - accuracy: 0.9509 - val_loss: 0.1777 - val_accuracy: 0.9315\n",
      "Epoch 139/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0894 - accuracy: 0.9550 - val_loss: 0.1414 - val_accuracy: 0.9383\n",
      "Epoch 140/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1017 - accuracy: 0.9525 - val_loss: 0.2175 - val_accuracy: 0.9258\n",
      "Epoch 141/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1089 - accuracy: 0.9464 - val_loss: 0.1547 - val_accuracy: 0.9382\n",
      "Epoch 142/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0910 - accuracy: 0.9537 - val_loss: 0.1757 - val_accuracy: 0.9318\n",
      "Epoch 143/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1135 - accuracy: 0.9464 - val_loss: 0.1763 - val_accuracy: 0.9248\n",
      "Epoch 144/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1035 - accuracy: 0.9481 - val_loss: 0.1698 - val_accuracy: 0.9336\n",
      "Epoch 145/500\n",
      "10/10 [==============================] - 5s 477ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1000 - accuracy: 0.9510 - val_loss: 0.1309 - val_accuracy: 0.9417\n",
      "Epoch 146/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0870 - accuracy: 0.9569 - val_loss: 0.1701 - val_accuracy: 0.9312\n",
      "Epoch 147/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0898 - accuracy: 0.9561 - val_loss: 0.1516 - val_accuracy: 0.9346\n",
      "Epoch 148/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0862 - accuracy: 0.9585 - val_loss: 0.1631 - val_accuracy: 0.9392\n",
      "Epoch 149/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0868 - accuracy: 0.9566 - val_loss: 0.1637 - val_accuracy: 0.9353\n",
      "Epoch 150/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0818 - accuracy: 0.9590 - val_loss: 0.1624 - val_accuracy: 0.9368\n",
      "Epoch 151/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0953 - accuracy: 0.9529 - val_loss: 0.1821 - val_accuracy: 0.9345\n",
      "Epoch 152/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0701 - accuracy: 0.9634 - val_loss: 0.1443 - val_accuracy: 0.9395\n",
      "Epoch 153/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0847 - accuracy: 0.9573 - val_loss: 0.1730 - val_accuracy: 0.9336\n",
      "Epoch 154/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0844 - accuracy: 0.9566 - val_loss: 0.2217 - val_accuracy: 0.9266\n",
      "Epoch 155/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0849 - accuracy: 0.9578 - val_loss: 0.1616 - val_accuracy: 0.9269\n",
      "Epoch 156/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0940 - accuracy: 0.9531 - val_loss: 0.1792 - val_accuracy: 0.9302\n",
      "Epoch 157/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0864 - accuracy: 0.9571 - val_loss: 0.1811 - val_accuracy: 0.9278\n",
      "Epoch 158/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1061 - accuracy: 0.9491 - val_loss: 0.1880 - val_accuracy: 0.9165\n",
      "Epoch 159/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1000 - accuracy: 0.9504 - val_loss: 0.1641 - val_accuracy: 0.9347\n",
      "Epoch 160/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0893 - accuracy: 0.9547 - val_loss: 0.1382 - val_accuracy: 0.9407\n",
      "Epoch 161/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0876 - accuracy: 0.9567 - val_loss: 0.1660 - val_accuracy: 0.9323\n",
      "Epoch 162/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0838 - accuracy: 0.9561 - val_loss: 0.1658 - val_accuracy: 0.9307\n",
      "Epoch 163/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0688 - accuracy: 0.9646 - val_loss: 0.1887 - val_accuracy: 0.9304\n",
      "Epoch 164/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0941 - accuracy: 0.9538 - val_loss: 0.1555 - val_accuracy: 0.9430\n",
      "Epoch 165/500\n",
      "10/10 [==============================] - 5s 477ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0785 - accuracy: 0.9599 - val_loss: 0.1440 - val_accuracy: 0.9397\n",
      "Epoch 166/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0756 - accuracy: 0.9622 - val_loss: 0.1801 - val_accuracy: 0.9316\n",
      "Epoch 167/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0654 - accuracy: 0.9664 - val_loss: 0.1698 - val_accuracy: 0.9340\n",
      "Epoch 168/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0925 - accuracy: 0.9523 - val_loss: 0.1549 - val_accuracy: 0.9411\n",
      "Epoch 169/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0975 - accuracy: 0.9524 - val_loss: 0.1504 - val_accuracy: 0.9396\n",
      "Epoch 170/500\n",
      "10/10 [==============================] - 5s 463ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0870 - accuracy: 0.9539 - val_loss: 0.1641 - val_accuracy: 0.9371\n",
      "Epoch 171/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0750 - accuracy: 0.9634 - val_loss: 0.1489 - val_accuracy: 0.9398\n",
      "Epoch 172/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0793 - accuracy: 0.9582 - val_loss: 0.1778 - val_accuracy: 0.9343\n",
      "Epoch 173/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0823 - accuracy: 0.9572 - val_loss: 0.1880 - val_accuracy: 0.9392\n",
      "Epoch 174/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0628 - accuracy: 0.9677 - val_loss: 0.1396 - val_accuracy: 0.9431\n",
      "Epoch 175/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0822 - accuracy: 0.9570 - val_loss: 0.1389 - val_accuracy: 0.9417\n",
      "Epoch 176/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0672 - accuracy: 0.9676 - val_loss: 0.1889 - val_accuracy: 0.9326\n",
      "Epoch 177/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0874 - accuracy: 0.9574 - val_loss: 0.1938 - val_accuracy: 0.9303\n",
      "Epoch 178/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0793 - accuracy: 0.9612 - val_loss: 0.2139 - val_accuracy: 0.9188\n",
      "Epoch 179/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0666 - accuracy: 0.9651 - val_loss: 0.2349 - val_accuracy: 0.9256\n",
      "Epoch 180/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0833 - accuracy: 0.9597 - val_loss: 0.1854 - val_accuracy: 0.9306\n",
      "Epoch 181/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0816 - accuracy: 0.9580 - val_loss: 0.1933 - val_accuracy: 0.9374\n",
      "Epoch 182/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0578 - accuracy: 0.9692 - val_loss: 0.1756 - val_accuracy: 0.9396\n",
      "Epoch 183/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0607 - accuracy: 0.9703 - val_loss: 0.1946 - val_accuracy: 0.9356\n",
      "Epoch 184/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0834 - accuracy: 0.9564 - val_loss: 0.1487 - val_accuracy: 0.9412\n",
      "Epoch 185/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0713 - accuracy: 0.9645 - val_loss: 0.1978 - val_accuracy: 0.9324\n",
      "Epoch 186/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0760 - accuracy: 0.9590 - val_loss: 0.2004 - val_accuracy: 0.9346\n",
      "Epoch 187/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0565 - accuracy: 0.9709 - val_loss: 0.1392 - val_accuracy: 0.9422\n",
      "Epoch 188/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0798 - accuracy: 0.9563 - val_loss: 0.1699 - val_accuracy: 0.9363\n",
      "Epoch 189/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0609 - accuracy: 0.9688 - val_loss: 0.2120 - val_accuracy: 0.9317\n",
      "Epoch 190/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0773 - accuracy: 0.9599 - val_loss: 0.1665 - val_accuracy: 0.9353\n",
      "Epoch 191/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0507 - accuracy: 0.9736 - val_loss: 0.1769 - val_accuracy: 0.9370\n",
      "Epoch 192/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0836 - accuracy: 0.9556 - val_loss: 0.1441 - val_accuracy: 0.9430\n",
      "Epoch 193/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0707 - accuracy: 0.9643 - val_loss: 0.1564 - val_accuracy: 0.9388\n",
      "Epoch 194/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0890 - accuracy: 0.9582 - val_loss: 0.1903 - val_accuracy: 0.9358\n",
      "Epoch 195/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1000 - accuracy: 0.9516 - val_loss: 0.1493 - val_accuracy: 0.9363\n",
      "Epoch 196/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0615 - accuracy: 0.9696 - val_loss: 0.1631 - val_accuracy: 0.9368\n",
      "Epoch 197/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0749 - accuracy: 0.9609 - val_loss: 0.1533 - val_accuracy: 0.9410\n",
      "Epoch 198/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0747 - accuracy: 0.9621 - val_loss: 0.1410 - val_accuracy: 0.9478\n",
      "Epoch 199/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0631 - accuracy: 0.9667 - val_loss: 0.2092 - val_accuracy: 0.9275\n",
      "Epoch 200/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0760 - accuracy: 0.9605 - val_loss: 0.1842 - val_accuracy: 0.9338\n",
      "Epoch 201/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0745 - accuracy: 0.9613 - val_loss: 0.1610 - val_accuracy: 0.9361\n",
      "Epoch 202/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0599 - accuracy: 0.9684 - val_loss: 0.1796 - val_accuracy: 0.9369\n",
      "Epoch 203/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0638 - accuracy: 0.9664 - val_loss: 0.2269 - val_accuracy: 0.9257\n",
      "Epoch 204/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0616 - accuracy: 0.9659 - val_loss: 0.1633 - val_accuracy: 0.9326\n",
      "Epoch 205/500\n",
      "10/10 [==============================] - 5s 479ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0715 - accuracy: 0.9620 - val_loss: 0.1947 - val_accuracy: 0.9322\n",
      "Epoch 206/500\n",
      "10/10 [==============================] - 5s 471ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0589 - accuracy: 0.9699 - val_loss: 0.1595 - val_accuracy: 0.9392\n",
      "Epoch 207/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0763 - accuracy: 0.9620 - val_loss: 0.2350 - val_accuracy: 0.9236\n",
      "Epoch 208/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0983 - accuracy: 0.9489 - val_loss: 0.1559 - val_accuracy: 0.9383\n",
      "Epoch 209/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0881 - accuracy: 0.9559 - val_loss: 0.2069 - val_accuracy: 0.9242\n",
      "Epoch 210/500\n",
      "10/10 [==============================] - 5s 463ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0708 - accuracy: 0.9623 - val_loss: 0.1818 - val_accuracy: 0.9325\n",
      "Epoch 211/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0707 - accuracy: 0.9645 - val_loss: 0.1773 - val_accuracy: 0.9366\n",
      "Epoch 212/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0598 - accuracy: 0.9681 - val_loss: 0.1976 - val_accuracy: 0.9288\n",
      "Epoch 213/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0631 - accuracy: 0.9669 - val_loss: 0.2522 - val_accuracy: 0.9288\n",
      "Epoch 214/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0615 - accuracy: 0.9676 - val_loss: 0.1907 - val_accuracy: 0.9311\n",
      "Epoch 215/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0597 - accuracy: 0.9677 - val_loss: 0.1770 - val_accuracy: 0.9368\n",
      "Epoch 216/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0567 - accuracy: 0.9688 - val_loss: 0.2055 - val_accuracy: 0.9336\n",
      "Epoch 217/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0525 - accuracy: 0.9705 - val_loss: 0.1927 - val_accuracy: 0.9318\n",
      "Epoch 218/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0644 - accuracy: 0.9670 - val_loss: 0.1386 - val_accuracy: 0.9436\n",
      "Epoch 219/500\n",
      "10/10 [==============================] - 5s 485ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0611 - accuracy: 0.9668 - val_loss: 0.2103 - val_accuracy: 0.9417\n",
      "Epoch 220/500\n",
      "10/10 [==============================] - 5s 463ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0559 - accuracy: 0.9687 - val_loss: 0.1892 - val_accuracy: 0.9343\n",
      "Epoch 221/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0710 - accuracy: 0.9621 - val_loss: 0.1607 - val_accuracy: 0.9418\n",
      "Epoch 222/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0534 - accuracy: 0.9727 - val_loss: 0.1976 - val_accuracy: 0.9320\n",
      "Epoch 223/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0695 - accuracy: 0.9621 - val_loss: 0.1514 - val_accuracy: 0.9498\n",
      "Epoch 224/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0545 - accuracy: 0.9725 - val_loss: 0.1884 - val_accuracy: 0.9341\n",
      "Epoch 225/500\n",
      "10/10 [==============================] - 5s 478ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0643 - accuracy: 0.9675 - val_loss: 0.2385 - val_accuracy: 0.9281\n",
      "Epoch 226/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0725 - accuracy: 0.9623 - val_loss: 0.2146 - val_accuracy: 0.9239\n",
      "Epoch 227/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0716 - accuracy: 0.9639 - val_loss: 0.1707 - val_accuracy: 0.9372\n",
      "Epoch 228/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0648 - accuracy: 0.9654 - val_loss: 0.2338 - val_accuracy: 0.9305\n",
      "Epoch 229/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0824 - accuracy: 0.9574 - val_loss: 0.2505 - val_accuracy: 0.9345\n",
      "Epoch 230/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0584 - accuracy: 0.9701 - val_loss: 0.2119 - val_accuracy: 0.9241\n",
      "Epoch 231/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0788 - accuracy: 0.9607 - val_loss: 0.1670 - val_accuracy: 0.9362\n",
      "Epoch 232/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0775 - accuracy: 0.9590 - val_loss: 0.1758 - val_accuracy: 0.9318\n",
      "Epoch 233/500\n",
      "10/10 [==============================] - 5s 485ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0750 - accuracy: 0.9604 - val_loss: 0.1552 - val_accuracy: 0.9353\n",
      "Epoch 234/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0657 - accuracy: 0.9673 - val_loss: 0.1839 - val_accuracy: 0.9311\n",
      "Epoch 235/500\n",
      "10/10 [==============================] - 5s 476ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0712 - accuracy: 0.9627 - val_loss: 0.1656 - val_accuracy: 0.9365\n",
      "Epoch 236/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0627 - accuracy: 0.9674 - val_loss: 0.1636 - val_accuracy: 0.9366\n",
      "Epoch 237/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0597 - accuracy: 0.9700 - val_loss: 0.1949 - val_accuracy: 0.9345\n",
      "Epoch 238/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0755 - accuracy: 0.9611 - val_loss: 0.1623 - val_accuracy: 0.9348\n",
      "Epoch 239/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0757 - accuracy: 0.9602 - val_loss: 0.1611 - val_accuracy: 0.9385\n",
      "Epoch 240/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0503 - accuracy: 0.9730 - val_loss: 0.2122 - val_accuracy: 0.9307\n",
      "Epoch 241/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0494 - accuracy: 0.9731 - val_loss: 0.1649 - val_accuracy: 0.9350\n",
      "Epoch 242/500\n",
      "10/10 [==============================] - 5s 471ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0723 - accuracy: 0.9630 - val_loss: 0.1655 - val_accuracy: 0.9373\n",
      "Epoch 243/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0597 - accuracy: 0.9687 - val_loss: 0.1792 - val_accuracy: 0.9306\n",
      "Epoch 244/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0645 - accuracy: 0.9656 - val_loss: 0.1493 - val_accuracy: 0.9380\n",
      "Epoch 245/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0481 - accuracy: 0.9738 - val_loss: 0.1737 - val_accuracy: 0.9358\n",
      "Epoch 246/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0599 - accuracy: 0.9661 - val_loss: 0.1657 - val_accuracy: 0.9374\n",
      "Epoch 247/500\n",
      "10/10 [==============================] - 5s 476ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0562 - accuracy: 0.9686 - val_loss: 0.1764 - val_accuracy: 0.9391\n",
      "Epoch 248/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0401 - accuracy: 0.9767 - val_loss: 0.1921 - val_accuracy: 0.9338\n",
      "Epoch 249/500\n",
      "10/10 [==============================] - 5s 487ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0434 - accuracy: 0.9766 - val_loss: 0.1729 - val_accuracy: 0.9380\n",
      "Epoch 250/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0635 - accuracy: 0.9641 - val_loss: 0.1878 - val_accuracy: 0.9372\n",
      "Epoch 251/500\n",
      "10/10 [==============================] - 5s 486ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0520 - accuracy: 0.9711 - val_loss: 0.2031 - val_accuracy: 0.9375\n",
      "Epoch 252/500\n",
      "10/10 [==============================] - 5s 473ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0482 - accuracy: 0.9720 - val_loss: 0.2227 - val_accuracy: 0.9301\n",
      "Epoch 253/500\n",
      "10/10 [==============================] - 5s 486ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0676 - accuracy: 0.9622 - val_loss: 0.1679 - val_accuracy: 0.9459\n",
      "Epoch 254/500\n",
      "10/10 [==============================] - 5s 471ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0447 - accuracy: 0.9749 - val_loss: 0.2258 - val_accuracy: 0.9289\n",
      "Epoch 255/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0768 - accuracy: 0.9592 - val_loss: 0.2428 - val_accuracy: 0.9300\n",
      "Epoch 256/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0765 - accuracy: 0.9620 - val_loss: 0.2145 - val_accuracy: 0.9209\n",
      "Epoch 257/500\n",
      "10/10 [==============================] - 5s 486ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0872 - accuracy: 0.9564 - val_loss: 0.2032 - val_accuracy: 0.9353\n",
      "Epoch 258/500\n",
      "10/10 [==============================] - 5s 472ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0634 - accuracy: 0.9671 - val_loss: 0.1922 - val_accuracy: 0.9353\n",
      "Epoch 259/500\n",
      "10/10 [==============================] - 5s 488ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1237 - accuracy: 0.9463 - val_loss: 0.3953 - val_accuracy: 0.8764\n",
      "Epoch 260/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1502 - accuracy: 0.9258 - val_loss: 0.6515 - val_accuracy: 0.8763\n",
      "Epoch 261/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1475 - accuracy: 0.9491 - val_loss: 0.1846 - val_accuracy: 0.9263\n",
      "Epoch 262/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1363 - accuracy: 0.9364 - val_loss: 0.1986 - val_accuracy: 0.9302\n",
      "Epoch 263/500\n",
      "10/10 [==============================] - 5s 486ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0947 - accuracy: 0.9543 - val_loss: 0.1624 - val_accuracy: 0.9330\n",
      "Epoch 264/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1026 - accuracy: 0.9538 - val_loss: 0.1994 - val_accuracy: 0.9267\n",
      "Epoch 265/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0788 - accuracy: 0.9610 - val_loss: 0.1556 - val_accuracy: 0.9360\n",
      "Epoch 266/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0839 - accuracy: 0.9581 - val_loss: 0.2181 - val_accuracy: 0.9337\n",
      "Epoch 267/500\n",
      "10/10 [==============================] - 5s 485ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0760 - accuracy: 0.9603 - val_loss: 0.1543 - val_accuracy: 0.9414\n",
      "Epoch 268/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0667 - accuracy: 0.9642 - val_loss: 0.2186 - val_accuracy: 0.9252\n",
      "Epoch 269/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0517 - accuracy: 0.9731 - val_loss: 0.1754 - val_accuracy: 0.9389\n",
      "Epoch 270/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0821 - accuracy: 0.9552 - val_loss: 0.1827 - val_accuracy: 0.9370\n",
      "Epoch 271/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0586 - accuracy: 0.9679 - val_loss: 0.1900 - val_accuracy: 0.9322\n",
      "Epoch 272/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0624 - accuracy: 0.9659 - val_loss: 0.1840 - val_accuracy: 0.9379\n",
      "Epoch 273/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0539 - accuracy: 0.9709 - val_loss: 0.2090 - val_accuracy: 0.9344\n",
      "Epoch 274/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0661 - accuracy: 0.9646 - val_loss: 0.1530 - val_accuracy: 0.9375\n",
      "Epoch 275/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0501 - accuracy: 0.9722 - val_loss: 0.1776 - val_accuracy: 0.9343\n",
      "Epoch 276/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1955 - accuracy: 0.9528 - val_loss: 0.1629 - val_accuracy: 0.9344\n",
      "Epoch 277/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0707 - accuracy: 0.9636 - val_loss: 0.1603 - val_accuracy: 0.9315\n",
      "Epoch 278/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0789 - accuracy: 0.9629 - val_loss: 0.1565 - val_accuracy: 0.9388\n",
      "Epoch 279/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0613 - accuracy: 0.9678 - val_loss: 0.2064 - val_accuracy: 0.9301\n",
      "Epoch 280/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0607 - accuracy: 0.9690 - val_loss: 0.2089 - val_accuracy: 0.9318\n",
      "Epoch 281/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0503 - accuracy: 0.9731 - val_loss: 0.1658 - val_accuracy: 0.9363\n",
      "Epoch 282/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1118 - accuracy: 0.9569 - val_loss: 0.1880 - val_accuracy: 0.9312\n",
      "Epoch 283/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0563 - accuracy: 0.9692 - val_loss: 0.1696 - val_accuracy: 0.9340\n",
      "Epoch 284/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0841 - accuracy: 0.9632 - val_loss: 0.1390 - val_accuracy: 0.9425\n",
      "Epoch 285/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0603 - accuracy: 0.9673 - val_loss: 0.2025 - val_accuracy: 0.9344\n",
      "Epoch 286/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0522 - accuracy: 0.9744 - val_loss: 0.1561 - val_accuracy: 0.9377\n",
      "Epoch 287/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0402 - accuracy: 0.9776 - val_loss: 0.1592 - val_accuracy: 0.9382\n",
      "Epoch 288/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0575 - accuracy: 0.9677 - val_loss: 0.1527 - val_accuracy: 0.9397\n",
      "Epoch 289/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0493 - accuracy: 0.9729 - val_loss: 0.2390 - val_accuracy: 0.9247\n",
      "Epoch 290/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0453 - accuracy: 0.9730 - val_loss: 0.1814 - val_accuracy: 0.9361\n",
      "Epoch 291/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0399 - accuracy: 0.9770 - val_loss: 0.1870 - val_accuracy: 0.9333\n",
      "Epoch 292/500\n",
      "10/10 [==============================] - 5s 471ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0577 - accuracy: 0.9683 - val_loss: 0.2101 - val_accuracy: 0.9351\n",
      "Epoch 293/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0500 - accuracy: 0.9712 - val_loss: 0.1844 - val_accuracy: 0.9319\n",
      "Epoch 294/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0449 - accuracy: 0.9747 - val_loss: 0.1832 - val_accuracy: 0.9408\n",
      "Epoch 295/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0462 - accuracy: 0.9734 - val_loss: 0.2541 - val_accuracy: 0.9270\n",
      "Epoch 296/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0554 - accuracy: 0.9695 - val_loss: 0.1821 - val_accuracy: 0.9340\n",
      "Epoch 297/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0512 - accuracy: 0.9704 - val_loss: 0.1641 - val_accuracy: 0.9383\n",
      "Epoch 298/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0418 - accuracy: 0.9754 - val_loss: 0.1988 - val_accuracy: 0.9309\n",
      "Epoch 299/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0429 - accuracy: 0.9744 - val_loss: 0.2012 - val_accuracy: 0.9327\n",
      "Epoch 300/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0460 - accuracy: 0.9730 - val_loss: 0.2351 - val_accuracy: 0.9294\n",
      "Epoch 301/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0483 - accuracy: 0.9715 - val_loss: 0.1806 - val_accuracy: 0.9352\n",
      "Epoch 302/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0432 - accuracy: 0.9745 - val_loss: 0.1427 - val_accuracy: 0.9463\n",
      "Epoch 303/500\n",
      "10/10 [==============================] - 5s 486ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0484 - accuracy: 0.9706 - val_loss: 0.2212 - val_accuracy: 0.9250\n",
      "Epoch 304/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0398 - accuracy: 0.9782 - val_loss: 0.1953 - val_accuracy: 0.9332\n",
      "Epoch 305/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0396 - accuracy: 0.9766 - val_loss: 0.2372 - val_accuracy: 0.9312\n",
      "Epoch 306/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0471 - accuracy: 0.9714 - val_loss: 0.1806 - val_accuracy: 0.9383\n",
      "Epoch 307/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0449 - accuracy: 0.9736 - val_loss: 0.1522 - val_accuracy: 0.9434\n",
      "Epoch 308/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0456 - accuracy: 0.9733 - val_loss: 0.2201 - val_accuracy: 0.9421\n",
      "Epoch 309/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0573 - accuracy: 0.9673 - val_loss: 0.2033 - val_accuracy: 0.9313\n",
      "Epoch 310/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0328 - accuracy: 0.9804 - val_loss: 0.1696 - val_accuracy: 0.9398\n",
      "Epoch 311/500\n",
      "10/10 [==============================] - 5s 485ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0450 - accuracy: 0.9734 - val_loss: 0.1890 - val_accuracy: 0.9341\n",
      "Epoch 312/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0403 - accuracy: 0.9752 - val_loss: 0.2425 - val_accuracy: 0.9376\n",
      "Epoch 313/500\n",
      "10/10 [==============================] - 5s 486ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0474 - accuracy: 0.9731 - val_loss: 0.2168 - val_accuracy: 0.9296\n",
      "Epoch 314/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0535 - accuracy: 0.9713 - val_loss: 0.1859 - val_accuracy: 0.9348\n",
      "Epoch 315/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0534 - accuracy: 0.9686 - val_loss: 0.2037 - val_accuracy: 0.9361\n",
      "Epoch 316/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0446 - accuracy: 0.9749 - val_loss: 0.2002 - val_accuracy: 0.9286\n",
      "Epoch 317/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0587 - accuracy: 0.9675 - val_loss: 0.2017 - val_accuracy: 0.9270\n",
      "Epoch 318/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0516 - accuracy: 0.9708 - val_loss: 0.1659 - val_accuracy: 0.9403\n",
      "Epoch 319/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0425 - accuracy: 0.9759 - val_loss: 0.1629 - val_accuracy: 0.9437\n",
      "Epoch 320/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0618 - accuracy: 0.9665 - val_loss: 0.1488 - val_accuracy: 0.9432\n",
      "Epoch 321/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0526 - accuracy: 0.9715 - val_loss: 0.1652 - val_accuracy: 0.9417\n",
      "Epoch 322/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0485 - accuracy: 0.9708 - val_loss: 0.1645 - val_accuracy: 0.9367\n",
      "Epoch 323/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0535 - accuracy: 0.9689 - val_loss: 0.1519 - val_accuracy: 0.9426\n",
      "Epoch 324/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0370 - accuracy: 0.9781 - val_loss: 0.1629 - val_accuracy: 0.9370\n",
      "Epoch 325/500\n",
      "10/10 [==============================] - 5s 478ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0378 - accuracy: 0.9774 - val_loss: 0.1969 - val_accuracy: 0.9357\n",
      "Epoch 326/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0515 - accuracy: 0.9715 - val_loss: 0.1800 - val_accuracy: 0.9388\n",
      "Epoch 327/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0442 - accuracy: 0.9742 - val_loss: 0.2146 - val_accuracy: 0.9382\n",
      "Epoch 328/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0395 - accuracy: 0.9759 - val_loss: 0.2662 - val_accuracy: 0.9274\n",
      "Epoch 329/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0387 - accuracy: 0.9768 - val_loss: 0.1892 - val_accuracy: 0.9365\n",
      "Epoch 330/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0476 - accuracy: 0.9711 - val_loss: 0.1870 - val_accuracy: 0.9367\n",
      "Epoch 331/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0373 - accuracy: 0.9781 - val_loss: 0.2279 - val_accuracy: 0.9365\n",
      "Epoch 332/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0492 - accuracy: 0.9707 - val_loss: 0.2089 - val_accuracy: 0.9373\n",
      "Epoch 333/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0402 - accuracy: 0.9751 - val_loss: 0.1906 - val_accuracy: 0.9394\n",
      "Epoch 334/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0406 - accuracy: 0.9761 - val_loss: 0.1523 - val_accuracy: 0.9455\n",
      "Epoch 335/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0333 - accuracy: 0.9792 - val_loss: 0.1731 - val_accuracy: 0.9401\n",
      "Epoch 336/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0429 - accuracy: 0.9733 - val_loss: 0.1811 - val_accuracy: 0.9353\n",
      "Epoch 337/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0368 - accuracy: 0.9769 - val_loss: 0.1886 - val_accuracy: 0.9390\n",
      "Epoch 338/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0396 - accuracy: 0.9742 - val_loss: 0.2183 - val_accuracy: 0.9330\n",
      "Epoch 339/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0485 - accuracy: 0.9717 - val_loss: 0.2244 - val_accuracy: 0.9334\n",
      "Epoch 340/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0372 - accuracy: 0.9781 - val_loss: 0.2059 - val_accuracy: 0.9434\n",
      "Epoch 341/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0475 - accuracy: 0.9713 - val_loss: 0.1570 - val_accuracy: 0.9385\n",
      "Epoch 342/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0385 - accuracy: 0.9773 - val_loss: 0.1883 - val_accuracy: 0.9365\n",
      "Epoch 343/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0434 - accuracy: 0.9733 - val_loss: 0.1848 - val_accuracy: 0.9374\n",
      "Epoch 344/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0357 - accuracy: 0.9791 - val_loss: 0.1715 - val_accuracy: 0.9397\n",
      "Epoch 345/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0417 - accuracy: 0.9746 - val_loss: 0.1566 - val_accuracy: 0.9431\n",
      "Epoch 346/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0466 - accuracy: 0.9749 - val_loss: 0.1838 - val_accuracy: 0.9368\n",
      "Epoch 347/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0492 - accuracy: 0.9712 - val_loss: 0.2320 - val_accuracy: 0.9338\n",
      "Epoch 348/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0359 - accuracy: 0.9774 - val_loss: 0.1874 - val_accuracy: 0.9349\n",
      "Epoch 349/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0305 - accuracy: 0.9806 - val_loss: 0.1913 - val_accuracy: 0.9351\n",
      "Epoch 350/500\n",
      "10/10 [==============================] - 4s 462ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0425 - accuracy: 0.9728 - val_loss: 0.2463 - val_accuracy: 0.9368\n",
      "Epoch 351/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0358 - accuracy: 0.9770 - val_loss: 0.2174 - val_accuracy: 0.9318\n",
      "Epoch 352/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0456 - accuracy: 0.9722 - val_loss: 0.2062 - val_accuracy: 0.9338\n",
      "Epoch 353/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0509 - accuracy: 0.9698 - val_loss: 0.2156 - val_accuracy: 0.9372\n",
      "Epoch 354/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0437 - accuracy: 0.9754 - val_loss: 0.2022 - val_accuracy: 0.9287\n",
      "Epoch 355/500\n",
      "10/10 [==============================] - 5s 478ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0363 - accuracy: 0.9766 - val_loss: 0.1914 - val_accuracy: 0.9381\n",
      "Epoch 356/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0463 - accuracy: 0.9730 - val_loss: 0.1893 - val_accuracy: 0.9380\n",
      "Epoch 357/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0328 - accuracy: 0.9786 - val_loss: 0.1728 - val_accuracy: 0.9439\n",
      "Epoch 358/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0464 - accuracy: 0.9736 - val_loss: 0.2063 - val_accuracy: 0.9284\n",
      "Epoch 359/500\n",
      "10/10 [==============================] - 5s 485ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0396 - accuracy: 0.9759 - val_loss: 0.2632 - val_accuracy: 0.9278\n",
      "Epoch 360/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0393 - accuracy: 0.9750 - val_loss: 0.2759 - val_accuracy: 0.9344\n",
      "Epoch 361/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0426 - accuracy: 0.9753 - val_loss: 0.1950 - val_accuracy: 0.9359\n",
      "Epoch 362/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0566 - accuracy: 0.9677 - val_loss: 0.2549 - val_accuracy: 0.9298\n",
      "Epoch 363/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0600 - accuracy: 0.9669 - val_loss: 0.2201 - val_accuracy: 0.9283\n",
      "Epoch 364/500\n",
      "10/10 [==============================] - 5s 471ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0389 - accuracy: 0.9767 - val_loss: 0.1840 - val_accuracy: 0.9391\n",
      "Epoch 365/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0422 - accuracy: 0.9732 - val_loss: 0.1994 - val_accuracy: 0.9387\n",
      "Epoch 366/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0369 - accuracy: 0.9782 - val_loss: 0.2242 - val_accuracy: 0.9388\n",
      "Epoch 367/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0434 - accuracy: 0.9739 - val_loss: 0.2071 - val_accuracy: 0.9335\n",
      "Epoch 368/500\n",
      "10/10 [==============================] - 5s 471ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0378 - accuracy: 0.9780 - val_loss: 0.2268 - val_accuracy: 0.9274\n",
      "Epoch 369/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0345 - accuracy: 0.9782 - val_loss: 0.2656 - val_accuracy: 0.9337\n",
      "Epoch 370/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0387 - accuracy: 0.9757 - val_loss: 0.2096 - val_accuracy: 0.9350\n",
      "Epoch 371/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0343 - accuracy: 0.9781 - val_loss: 0.2244 - val_accuracy: 0.9370\n",
      "Epoch 372/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0361 - accuracy: 0.9773 - val_loss: 0.1882 - val_accuracy: 0.9356\n",
      "Epoch 373/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0357 - accuracy: 0.9769 - val_loss: 0.1615 - val_accuracy: 0.9437\n",
      "Epoch 374/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1775 - accuracy: 0.9585 - val_loss: 0.1702 - val_accuracy: 0.9294\n",
      "Epoch 375/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0485 - accuracy: 0.9746 - val_loss: 0.1706 - val_accuracy: 0.9352\n",
      "Epoch 376/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2063 - accuracy: 0.9439 - val_loss: 0.1656 - val_accuracy: 0.9313\n",
      "Epoch 377/500\n",
      "10/10 [==============================] - 5s 486ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0948 - accuracy: 0.9546 - val_loss: 0.2900 - val_accuracy: 0.9042\n",
      "Epoch 378/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0735 - accuracy: 0.9639 - val_loss: 0.1175 - val_accuracy: 0.9467\n",
      "Epoch 379/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0743 - accuracy: 0.9620 - val_loss: 0.1579 - val_accuracy: 0.9396\n",
      "Epoch 380/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0729 - accuracy: 0.9639 - val_loss: 0.1582 - val_accuracy: 0.9370\n",
      "Epoch 381/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0574 - accuracy: 0.9683 - val_loss: 0.1753 - val_accuracy: 0.9376\n",
      "Epoch 382/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0377 - accuracy: 0.9780 - val_loss: 0.1516 - val_accuracy: 0.9407\n",
      "Epoch 383/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0424 - accuracy: 0.9757 - val_loss: 0.1819 - val_accuracy: 0.9329\n",
      "Epoch 384/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0444 - accuracy: 0.9726 - val_loss: 0.1782 - val_accuracy: 0.9317\n",
      "Epoch 385/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0393 - accuracy: 0.9761 - val_loss: 0.1707 - val_accuracy: 0.9381\n",
      "Epoch 386/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0408 - accuracy: 0.9761 - val_loss: 0.1808 - val_accuracy: 0.9352\n",
      "Epoch 387/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0340 - accuracy: 0.9784 - val_loss: 0.2009 - val_accuracy: 0.9355\n",
      "Epoch 388/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0378 - accuracy: 0.9757 - val_loss: 0.1923 - val_accuracy: 0.9340\n",
      "Epoch 389/500\n",
      "10/10 [==============================] - 5s 485ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0390 - accuracy: 0.9752 - val_loss: 0.2276 - val_accuracy: 0.9260\n",
      "Epoch 390/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0308 - accuracy: 0.9793 - val_loss: 0.1643 - val_accuracy: 0.9403\n",
      "Epoch 391/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0344 - accuracy: 0.9779 - val_loss: 0.1979 - val_accuracy: 0.9359\n",
      "Epoch 392/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0350 - accuracy: 0.9775 - val_loss: 0.1853 - val_accuracy: 0.9425\n",
      "Epoch 393/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0366 - accuracy: 0.9758 - val_loss: 0.2501 - val_accuracy: 0.9295\n",
      "Epoch 394/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0341 - accuracy: 0.9790 - val_loss: 0.2112 - val_accuracy: 0.9347\n",
      "Epoch 395/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0394 - accuracy: 0.9753 - val_loss: 0.1916 - val_accuracy: 0.9367\n",
      "Epoch 396/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2055 - accuracy: 0.9493 - val_loss: 0.1710 - val_accuracy: 0.9386\n",
      "Epoch 397/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0567 - accuracy: 0.9722 - val_loss: 0.1482 - val_accuracy: 0.9361\n",
      "Epoch 398/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0487 - accuracy: 0.9718 - val_loss: 0.1789 - val_accuracy: 0.9336\n",
      "Epoch 399/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0385 - accuracy: 0.9784 - val_loss: 0.1791 - val_accuracy: 0.9336\n",
      "Epoch 400/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0412 - accuracy: 0.9737 - val_loss: 0.1667 - val_accuracy: 0.9381\n",
      "Epoch 401/500\n",
      "10/10 [==============================] - 5s 486ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0407 - accuracy: 0.9746 - val_loss: 0.1793 - val_accuracy: 0.9368\n",
      "Epoch 402/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0409 - accuracy: 0.9753 - val_loss: 0.1693 - val_accuracy: 0.9402\n",
      "Epoch 403/500\n",
      "10/10 [==============================] - 5s 478ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0334 - accuracy: 0.9788 - val_loss: 0.1763 - val_accuracy: 0.9432\n",
      "Epoch 404/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0446 - accuracy: 0.9734 - val_loss: 0.1856 - val_accuracy: 0.9355\n",
      "Epoch 405/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0372 - accuracy: 0.9768 - val_loss: 0.1765 - val_accuracy: 0.9378\n",
      "Epoch 406/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0360 - accuracy: 0.9770 - val_loss: 0.1948 - val_accuracy: 0.9369\n",
      "Epoch 407/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0350 - accuracy: 0.9770 - val_loss: 0.2045 - val_accuracy: 0.9373\n",
      "Epoch 408/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0311 - accuracy: 0.9803 - val_loss: 0.1393 - val_accuracy: 0.9495\n",
      "Epoch 409/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0347 - accuracy: 0.9770 - val_loss: 0.1568 - val_accuracy: 0.9417\n",
      "Epoch 410/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0332 - accuracy: 0.9783 - val_loss: 0.1561 - val_accuracy: 0.9459\n",
      "Epoch 411/500\n",
      "10/10 [==============================] - 5s 486ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0347 - accuracy: 0.9762 - val_loss: 0.1966 - val_accuracy: 0.9381\n",
      "Epoch 412/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0273 - accuracy: 0.9819 - val_loss: 0.2056 - val_accuracy: 0.9339\n",
      "Epoch 413/500\n",
      "10/10 [==============================] - 5s 490ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0309 - accuracy: 0.9790 - val_loss: 0.2020 - val_accuracy: 0.9351\n",
      "Epoch 414/500\n",
      "10/10 [==============================] - 5s 474ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0332 - accuracy: 0.9783 - val_loss: 0.2376 - val_accuracy: 0.9394\n",
      "Epoch 415/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0517 - accuracy: 0.9722 - val_loss: 0.1655 - val_accuracy: 0.9393\n",
      "Epoch 416/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0348 - accuracy: 0.9798 - val_loss: 0.2299 - val_accuracy: 0.9337\n",
      "Epoch 417/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0317 - accuracy: 0.9797 - val_loss: 0.2121 - val_accuracy: 0.9310\n",
      "Epoch 418/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0379 - accuracy: 0.9753 - val_loss: 0.1982 - val_accuracy: 0.9344\n",
      "Epoch 419/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2330 - accuracy: 0.9418 - val_loss: 0.1675 - val_accuracy: 0.9279\n",
      "Epoch 420/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0775 - accuracy: 0.9626 - val_loss: 0.1393 - val_accuracy: 0.9425\n",
      "Epoch 421/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0611 - accuracy: 0.9674 - val_loss: 0.1422 - val_accuracy: 0.9377\n",
      "Epoch 422/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0502 - accuracy: 0.9726 - val_loss: 0.1739 - val_accuracy: 0.9362\n",
      "Epoch 423/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0452 - accuracy: 0.9733 - val_loss: 0.1932 - val_accuracy: 0.9374\n",
      "Epoch 424/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0444 - accuracy: 0.9749 - val_loss: 0.1893 - val_accuracy: 0.9321\n",
      "Epoch 425/500\n",
      "10/10 [==============================] - 5s 485ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0347 - accuracy: 0.9787 - val_loss: 0.1424 - val_accuracy: 0.9443\n",
      "Epoch 426/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0413 - accuracy: 0.9746 - val_loss: 0.1682 - val_accuracy: 0.9394\n",
      "Epoch 427/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0327 - accuracy: 0.9794 - val_loss: 0.1761 - val_accuracy: 0.9369\n",
      "Epoch 428/500\n",
      "10/10 [==============================] - 5s 473ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0433 - accuracy: 0.9732 - val_loss: 0.1901 - val_accuracy: 0.9329\n",
      "Epoch 429/500\n",
      "10/10 [==============================] - 5s 486ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0452 - accuracy: 0.9725 - val_loss: 0.1494 - val_accuracy: 0.9423\n",
      "Epoch 430/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0321 - accuracy: 0.9799 - val_loss: 0.1805 - val_accuracy: 0.9372\n",
      "Epoch 431/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0372 - accuracy: 0.9763 - val_loss: 0.1915 - val_accuracy: 0.9377\n",
      "Epoch 432/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0377 - accuracy: 0.9773 - val_loss: 0.1837 - val_accuracy: 0.9368\n",
      "Epoch 433/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0337 - accuracy: 0.9783 - val_loss: 0.1814 - val_accuracy: 0.9376\n",
      "Epoch 434/500\n",
      "10/10 [==============================] - 5s 477ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0402 - accuracy: 0.9755 - val_loss: 0.1649 - val_accuracy: 0.9430\n",
      "Epoch 435/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0350 - accuracy: 0.9769 - val_loss: 0.1858 - val_accuracy: 0.9386\n",
      "Epoch 436/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1589 - accuracy: 0.9521 - val_loss: 0.1506 - val_accuracy: 0.9403\n",
      "Epoch 437/500\n",
      "10/10 [==============================] - 5s 486ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0681 - accuracy: 0.9643 - val_loss: 0.1476 - val_accuracy: 0.9417\n",
      "Epoch 438/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0685 - accuracy: 0.9648 - val_loss: 0.1707 - val_accuracy: 0.9335\n",
      "Epoch 439/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0493 - accuracy: 0.9733 - val_loss: 0.2122 - val_accuracy: 0.9218\n",
      "Epoch 440/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0780 - accuracy: 0.9604 - val_loss: 0.1609 - val_accuracy: 0.9393\n",
      "Epoch 441/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0498 - accuracy: 0.9729 - val_loss: 0.1986 - val_accuracy: 0.9303\n",
      "Epoch 442/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0471 - accuracy: 0.9717 - val_loss: 0.1896 - val_accuracy: 0.9366\n",
      "Epoch 443/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0420 - accuracy: 0.9745 - val_loss: 0.1622 - val_accuracy: 0.9399\n",
      "Epoch 444/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0333 - accuracy: 0.9785 - val_loss: 0.1778 - val_accuracy: 0.9428\n",
      "Epoch 445/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0298 - accuracy: 0.9805 - val_loss: 0.1956 - val_accuracy: 0.9342\n",
      "Epoch 446/500\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0384 - accuracy: 0.9753 - val_loss: 0.1957 - val_accuracy: 0.9356\n",
      "Epoch 447/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0346 - accuracy: 0.9775 - val_loss: 0.2100 - val_accuracy: 0.9356\n",
      "Epoch 448/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0310 - accuracy: 0.9791 - val_loss: 0.1960 - val_accuracy: 0.9398\n",
      "Epoch 449/500\n",
      "10/10 [==============================] - 5s 485ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0269 - accuracy: 0.9814 - val_loss: 0.1665 - val_accuracy: 0.9428\n",
      "Epoch 450/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0365 - accuracy: 0.9764 - val_loss: 0.1901 - val_accuracy: 0.9370\n",
      "Epoch 451/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0365 - accuracy: 0.9753 - val_loss: 0.1622 - val_accuracy: 0.9452\n",
      "Epoch 452/500\n",
      "10/10 [==============================] - 5s 472ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0272 - accuracy: 0.9827 - val_loss: 0.1859 - val_accuracy: 0.9380\n",
      "Epoch 453/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0321 - accuracy: 0.9779 - val_loss: 0.1965 - val_accuracy: 0.9393\n",
      "Epoch 454/500\n",
      "10/10 [==============================] - 5s 473ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0277 - accuracy: 0.9804 - val_loss: 0.1813 - val_accuracy: 0.9387\n",
      "Epoch 455/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0323 - accuracy: 0.9772 - val_loss: 0.1973 - val_accuracy: 0.9368\n",
      "Epoch 456/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0279 - accuracy: 0.9813 - val_loss: 0.2254 - val_accuracy: 0.9336\n",
      "Epoch 457/500\n",
      "10/10 [==============================] - 5s 485ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0316 - accuracy: 0.9777 - val_loss: 0.1973 - val_accuracy: 0.9355\n",
      "Epoch 458/500\n",
      "10/10 [==============================] - 5s 471ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0261 - accuracy: 0.9813 - val_loss: 0.1925 - val_accuracy: 0.9383\n",
      "Epoch 459/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0253 - accuracy: 0.9812 - val_loss: 0.1925 - val_accuracy: 0.9377\n",
      "Epoch 460/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0323 - accuracy: 0.9777 - val_loss: 0.2004 - val_accuracy: 0.9363\n",
      "Epoch 461/500\n",
      "10/10 [==============================] - 5s 480ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0356 - accuracy: 0.9754 - val_loss: 0.1859 - val_accuracy: 0.9396\n",
      "Epoch 462/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0238 - accuracy: 0.9830 - val_loss: 0.1951 - val_accuracy: 0.9362\n",
      "Epoch 463/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0312 - accuracy: 0.9785 - val_loss: 0.1979 - val_accuracy: 0.9380\n",
      "Epoch 464/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0256 - accuracy: 0.9808 - val_loss: 0.1967 - val_accuracy: 0.9404\n",
      "Epoch 465/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0302 - accuracy: 0.9791 - val_loss: 0.1638 - val_accuracy: 0.9448\n",
      "Epoch 466/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0259 - accuracy: 0.9802 - val_loss: 0.2046 - val_accuracy: 0.9407\n",
      "Epoch 467/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0320 - accuracy: 0.9775 - val_loss: 0.1990 - val_accuracy: 0.9380\n",
      "Epoch 468/500\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0248 - accuracy: 0.9822 - val_loss: 0.2000 - val_accuracy: 0.9370\n",
      "Epoch 469/500\n",
      "10/10 [==============================] - 5s 485ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0347 - accuracy: 0.9749 - val_loss: 0.2517 - val_accuracy: 0.9327\n",
      "Epoch 470/500\n",
      "10/10 [==============================] - 5s 472ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0246 - accuracy: 0.9837 - val_loss: 0.2329 - val_accuracy: 0.9371\n",
      "Epoch 471/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0296 - accuracy: 0.9797 - val_loss: 0.1709 - val_accuracy: 0.9401\n",
      "Epoch 472/500\n",
      "10/10 [==============================] - 5s 466ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0299 - accuracy: 0.9784 - val_loss: 0.2119 - val_accuracy: 0.9395\n",
      "Epoch 473/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0265 - accuracy: 0.9812 - val_loss: 0.2074 - val_accuracy: 0.9367\n",
      "Epoch 474/500\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0326 - accuracy: 0.9780 - val_loss: 0.2089 - val_accuracy: 0.9384\n",
      "Epoch 475/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0312 - accuracy: 0.9776 - val_loss: 0.1842 - val_accuracy: 0.9408\n",
      "Epoch 476/500\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0244 - accuracy: 0.9828 - val_loss: 0.1739 - val_accuracy: 0.9431\n",
      "Epoch 477/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0297 - accuracy: 0.9786 - val_loss: 0.2252 - val_accuracy: 0.9378\n",
      "Epoch 478/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0278 - accuracy: 0.9801 - val_loss: 0.2050 - val_accuracy: 0.9397\n",
      "Epoch 479/500\n",
      "10/10 [==============================] - 5s 483ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0281 - accuracy: 0.9797 - val_loss: 0.1619 - val_accuracy: 0.9466\n",
      "Epoch 480/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0281 - accuracy: 0.9804 - val_loss: 0.1939 - val_accuracy: 0.9418\n",
      "Epoch 481/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0298 - accuracy: 0.9782 - val_loss: 0.2014 - val_accuracy: 0.9393\n",
      "Epoch 482/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0242 - accuracy: 0.9818 - val_loss: 0.2010 - val_accuracy: 0.9375\n",
      "Epoch 483/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0291 - accuracy: 0.9791 - val_loss: 0.1972 - val_accuracy: 0.9371\n",
      "Epoch 484/500\n",
      "10/10 [==============================] - 5s 471ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0253 - accuracy: 0.9816 - val_loss: 0.1804 - val_accuracy: 0.9431\n",
      "Epoch 485/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0274 - accuracy: 0.9799 - val_loss: 0.2031 - val_accuracy: 0.9424\n",
      "Epoch 486/500\n",
      "10/10 [==============================] - 5s 471ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0267 - accuracy: 0.9802 - val_loss: 0.2826 - val_accuracy: 0.9288\n",
      "Epoch 487/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0320 - accuracy: 0.9780 - val_loss: 0.2274 - val_accuracy: 0.9352\n",
      "Epoch 488/500\n",
      "10/10 [==============================] - 5s 471ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0326 - accuracy: 0.9787 - val_loss: 0.2397 - val_accuracy: 0.9304\n",
      "Epoch 489/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0304 - accuracy: 0.9800 - val_loss: 0.1835 - val_accuracy: 0.9499\n",
      "Epoch 490/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0331 - accuracy: 0.9771 - val_loss: 0.1853 - val_accuracy: 0.9384\n",
      "Epoch 491/500\n",
      "10/10 [==============================] - 5s 481ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0316 - accuracy: 0.9787 - val_loss: 0.1915 - val_accuracy: 0.9416\n",
      "Epoch 492/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0289 - accuracy: 0.9800 - val_loss: 0.2239 - val_accuracy: 0.9366\n",
      "Epoch 493/500\n",
      "10/10 [==============================] - 5s 487ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0308 - accuracy: 0.9794 - val_loss: 0.2350 - val_accuracy: 0.9367\n",
      "Epoch 494/500\n",
      "10/10 [==============================] - 5s 471ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0348 - accuracy: 0.9769 - val_loss: 0.1467 - val_accuracy: 0.9506\n",
      "Epoch 495/500\n",
      "10/10 [==============================] - 5s 485ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0258 - accuracy: 0.9821 - val_loss: 0.2192 - val_accuracy: 0.9397\n",
      "Epoch 496/500\n",
      "10/10 [==============================] - 5s 468ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0344 - accuracy: 0.9752 - val_loss: 0.1893 - val_accuracy: 0.9388\n",
      "Epoch 497/500\n",
      "10/10 [==============================] - 5s 484ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0258 - accuracy: 0.9812 - val_loss: 0.2189 - val_accuracy: 0.9409\n",
      "Epoch 498/500\n",
      "10/10 [==============================] - 5s 469ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0302 - accuracy: 0.9780 - val_loss: 0.1536 - val_accuracy: 0.9503\n",
      "Epoch 499/500\n",
      "10/10 [==============================] - 5s 482ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.0295 - accuracy: 0.9777 - val_loss: 0.1855 - val_accuracy: 0.9420\n",
      "Epoch 500/500\n",
      "10/10 [==============================] - 5s 471ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.0245 - accuracy: 0.9827 - val_loss: 0.2026 - val_accuracy: 0.9384\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#显示损失曲线\n",
    "history = LossHistory()\n",
    "model.fit_generator(myGene,steps_per_epoch=10,epochs=500,callbacks=[history],validation_data = valid_data, validation_steps = 10) #epoch循环次数\n",
    "#acc-loss曲线绘制\n",
    "history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 557752,
     "status": "ok",
     "timestamp": 1630398879816,
     "user": {
      "displayName": "sijin li",
      "photoUrl": "",
      "userId": "04019906230177487561"
     },
     "user_tz": -480
    },
    "id": "TnlaUnJcXQwv",
    "outputId": "b3be08e5-80bb-443c-e0ef-7f47cd1de5f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:1228: UserWarning: `model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 images belonging to 1 classes.\n",
      "<keras.preprocessing.image.DirectoryIterator object at 0x7f254b91ad10>\n",
      "Found 39 images belonging to 1 classes.\n",
      "Epoch 1/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.7010 - accuracy: 0.5480\n",
      "Epoch 00001: loss improved from inf to 0.70097, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 10s 733ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.7010 - accuracy: 0.5480\n",
      "Epoch 2/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.6723 - accuracy: 0.6367\n",
      "Epoch 00002: loss improved from 0.70097 to 0.67019, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 5s 475ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.6721 - accuracy: 0.6367\n",
      "Epoch 3/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.6422 - accuracy: 0.7271\n",
      "Epoch 00003: loss improved from 0.67019 to 0.64223, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 453ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.6422 - accuracy: 0.7271\n",
      "Epoch 4/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.6280 - accuracy: 0.7713\n",
      "Epoch 00004: loss improved from 0.64223 to 0.62310, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 445ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.6275 - accuracy: 0.7713\n",
      "Epoch 5/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.6058 - accuracy: 0.7706\n",
      "Epoch 00005: loss improved from 0.62310 to 0.60579, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 461ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.6058 - accuracy: 0.7706\n",
      "Epoch 6/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.6077 - accuracy: 0.7608\n",
      "Epoch 00006: loss improved from 0.60579 to 0.60437, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 446ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.6074 - accuracy: 0.7608\n",
      "Epoch 7/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.5860 - accuracy: 0.7756\n",
      "Epoch 00007: loss improved from 0.60437 to 0.58603, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 461ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.5860 - accuracy: 0.7756\n",
      "Epoch 8/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.5569 - accuracy: 0.7761\n",
      "Epoch 00008: loss improved from 0.58603 to 0.55260, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 439ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.5565 - accuracy: 0.7761\n",
      "Epoch 9/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.5394 - accuracy: 0.7765\n",
      "Epoch 00009: loss improved from 0.55260 to 0.53943, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 5s 470ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.5394 - accuracy: 0.7765\n",
      "Epoch 10/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.5119 - accuracy: 0.7921\n",
      "Epoch 00010: loss improved from 0.53943 to 0.51537, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 445ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.5122 - accuracy: 0.7921\n",
      "Epoch 11/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.4672 - accuracy: 0.8154\n",
      "Epoch 00011: loss improved from 0.51537 to 0.46717, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 460ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.4672 - accuracy: 0.8154\n",
      "Epoch 12/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.5045 - accuracy: 0.7994\n",
      "Epoch 00012: loss did not improve from 0.46717\n",
      "10/10 [==============================] - 3s 323ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.5044 - accuracy: 0.7994\n",
      "Epoch 13/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.4797 - accuracy: 0.7787\n",
      "Epoch 00013: loss did not improve from 0.46717\n",
      "10/10 [==============================] - 3s 338ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.4797 - accuracy: 0.7787\n",
      "Epoch 14/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.4603 - accuracy: 0.7880\n",
      "Epoch 00014: loss improved from 0.46717 to 0.46603, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 454ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.4608 - accuracy: 0.7880\n",
      "Epoch 15/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.3597 - accuracy: 0.8477\n",
      "Epoch 00015: loss improved from 0.46603 to 0.35967, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 460ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3597 - accuracy: 0.8477\n",
      "Epoch 16/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.4060 - accuracy: 0.7998\n",
      "Epoch 00016: loss did not improve from 0.35967\n",
      "10/10 [==============================] - 3s 323ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.4063 - accuracy: 0.7998\n",
      "Epoch 17/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.3708 - accuracy: 0.8219\n",
      "Epoch 00017: loss did not improve from 0.35967\n",
      "10/10 [==============================] - 3s 339ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3708 - accuracy: 0.8219\n",
      "Epoch 18/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.3150 - accuracy: 0.8459\n",
      "Epoch 00018: loss improved from 0.35967 to 0.31641, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 454ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.3151 - accuracy: 0.8459\n",
      "Epoch 19/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2794 - accuracy: 0.8670\n",
      "Epoch 00019: loss improved from 0.31641 to 0.27943, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 5s 499ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2794 - accuracy: 0.8670\n",
      "Epoch 20/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.3578 - accuracy: 0.8275\n",
      "Epoch 00020: loss did not improve from 0.27943\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.3576 - accuracy: 0.8275\n",
      "Epoch 21/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.3509 - accuracy: 0.8307\n",
      "Epoch 00021: loss did not improve from 0.27943\n",
      "10/10 [==============================] - 3s 338ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3509 - accuracy: 0.8307\n",
      "Epoch 22/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2914 - accuracy: 0.8389\n",
      "Epoch 00022: loss did not improve from 0.27943\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2926 - accuracy: 0.8389\n",
      "Epoch 23/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.3273 - accuracy: 0.8293\n",
      "Epoch 00023: loss did not improve from 0.27943\n",
      "10/10 [==============================] - 3s 341ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3273 - accuracy: 0.8293\n",
      "Epoch 24/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.3010 - accuracy: 0.8576\n",
      "Epoch 00024: loss did not improve from 0.27943\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2999 - accuracy: 0.8576\n",
      "Epoch 25/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.3367 - accuracy: 0.8242\n",
      "Epoch 00025: loss did not improve from 0.27943\n",
      "10/10 [==============================] - 3s 339ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3367 - accuracy: 0.8242\n",
      "Epoch 26/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.3022 - accuracy: 0.8545\n",
      "Epoch 00026: loss did not improve from 0.27943\n",
      "10/10 [==============================] - 3s 324ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.3020 - accuracy: 0.8545\n",
      "Epoch 27/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.3204 - accuracy: 0.8388\n",
      "Epoch 00027: loss did not improve from 0.27943\n",
      "10/10 [==============================] - 3s 339ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3204 - accuracy: 0.8388\n",
      "Epoch 28/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.3025 - accuracy: 0.8526\n",
      "Epoch 00028: loss did not improve from 0.27943\n",
      "10/10 [==============================] - 3s 325ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.3016 - accuracy: 0.8526\n",
      "Epoch 29/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.3235 - accuracy: 0.8358\n",
      "Epoch 00029: loss did not improve from 0.27943\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3235 - accuracy: 0.8358\n",
      "Epoch 30/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2930 - accuracy: 0.8443\n",
      "Epoch 00030: loss did not improve from 0.27943\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2930 - accuracy: 0.8443\n",
      "Epoch 31/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2751 - accuracy: 0.8722\n",
      "Epoch 00031: loss improved from 0.27943 to 0.27508, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 6s 631ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2751 - accuracy: 0.8722\n",
      "Epoch 32/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2980 - accuracy: 0.8411\n",
      "Epoch 00032: loss did not improve from 0.27508\n",
      "10/10 [==============================] - 3s 332ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2994 - accuracy: 0.8411\n",
      "Epoch 33/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2700 - accuracy: 0.8716\n",
      "Epoch 00033: loss improved from 0.27508 to 0.26999, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 5s 465ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2700 - accuracy: 0.8716\n",
      "Epoch 34/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2832 - accuracy: 0.8719\n",
      "Epoch 00034: loss did not improve from 0.26999\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2825 - accuracy: 0.8719\n",
      "Epoch 35/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.3270 - accuracy: 0.8358\n",
      "Epoch 00035: loss did not improve from 0.26999\n",
      "10/10 [==============================] - 3s 341ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3270 - accuracy: 0.8358\n",
      "Epoch 36/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.3143 - accuracy: 0.8468\n",
      "Epoch 00036: loss did not improve from 0.26999\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.3143 - accuracy: 0.8468\n",
      "Epoch 37/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.3397 - accuracy: 0.8126\n",
      "Epoch 00037: loss did not improve from 0.26999\n",
      "10/10 [==============================] - 3s 339ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.3397 - accuracy: 0.8126\n",
      "Epoch 38/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2597 - accuracy: 0.8738\n",
      "Epoch 00038: loss improved from 0.26999 to 0.26379, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 6s 602ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2601 - accuracy: 0.8738\n",
      "Epoch 39/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2878 - accuracy: 0.8575\n",
      "Epoch 00039: loss did not improve from 0.26379\n",
      "10/10 [==============================] - 3s 348ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2878 - accuracy: 0.8575\n",
      "Epoch 40/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2456 - accuracy: 0.8786\n",
      "Epoch 00040: loss improved from 0.26379 to 0.25589, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 456ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2465 - accuracy: 0.8786\n",
      "Epoch 41/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2819 - accuracy: 0.8639\n",
      "Epoch 00041: loss did not improve from 0.25589\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2819 - accuracy: 0.8639\n",
      "Epoch 42/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2516 - accuracy: 0.8772\n",
      "Epoch 00042: loss improved from 0.25589 to 0.25418, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 452ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2518 - accuracy: 0.8772\n",
      "Epoch 43/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2344 - accuracy: 0.8896\n",
      "Epoch 00043: loss improved from 0.25418 to 0.23437, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 5s 464ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2344 - accuracy: 0.8896\n",
      "Epoch 44/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.3083 - accuracy: 0.8513\n",
      "Epoch 00044: loss did not improve from 0.23437\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.3075 - accuracy: 0.8513\n",
      "Epoch 45/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2401 - accuracy: 0.8869\n",
      "Epoch 00045: loss did not improve from 0.23437\n",
      "10/10 [==============================] - 3s 343ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2401 - accuracy: 0.8869\n",
      "Epoch 46/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2789 - accuracy: 0.8656\n",
      "Epoch 00046: loss did not improve from 0.23437\n",
      "10/10 [==============================] - 3s 328ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2784 - accuracy: 0.8656\n",
      "Epoch 47/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2158 - accuracy: 0.8917\n",
      "Epoch 00047: loss improved from 0.23437 to 0.21582, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 6s 613ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2158 - accuracy: 0.8917\n",
      "Epoch 48/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.3276 - accuracy: 0.8316\n",
      "Epoch 00048: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 333ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.3283 - accuracy: 0.8316\n",
      "Epoch 49/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2483 - accuracy: 0.8790\n",
      "Epoch 00049: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2483 - accuracy: 0.8790\n",
      "Epoch 50/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2663 - accuracy: 0.8650\n",
      "Epoch 00050: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2665 - accuracy: 0.8650\n",
      "Epoch 51/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2588 - accuracy: 0.8712\n",
      "Epoch 00051: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 340ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2588 - accuracy: 0.8712\n",
      "Epoch 52/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2813 - accuracy: 0.8616\n",
      "Epoch 00052: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2806 - accuracy: 0.8616\n",
      "Epoch 53/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2398 - accuracy: 0.8901\n",
      "Epoch 00053: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 340ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2398 - accuracy: 0.8901\n",
      "Epoch 54/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2792 - accuracy: 0.8690\n",
      "Epoch 00054: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 328ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2793 - accuracy: 0.8690\n",
      "Epoch 55/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2338 - accuracy: 0.8866\n",
      "Epoch 00055: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 343ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2338 - accuracy: 0.8866\n",
      "Epoch 56/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2574 - accuracy: 0.8761\n",
      "Epoch 00056: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 328ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2574 - accuracy: 0.8761\n",
      "Epoch 57/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2782 - accuracy: 0.8677\n",
      "Epoch 00057: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2782 - accuracy: 0.8677\n",
      "Epoch 58/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2440 - accuracy: 0.8821\n",
      "Epoch 00058: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 328ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2448 - accuracy: 0.8821\n",
      "Epoch 59/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2481 - accuracy: 0.8902\n",
      "Epoch 00059: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 341ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2481 - accuracy: 0.8902\n",
      "Epoch 60/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2893 - accuracy: 0.8607\n",
      "Epoch 00060: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 329ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2894 - accuracy: 0.8607\n",
      "Epoch 61/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2689 - accuracy: 0.8672\n",
      "Epoch 00061: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 343ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2689 - accuracy: 0.8672\n",
      "Epoch 62/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2430 - accuracy: 0.8823\n",
      "Epoch 00062: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2429 - accuracy: 0.8823\n",
      "Epoch 63/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2631 - accuracy: 0.8704\n",
      "Epoch 00063: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 341ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2631 - accuracy: 0.8704\n",
      "Epoch 64/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2659 - accuracy: 0.8629\n",
      "Epoch 00064: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 328ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2671 - accuracy: 0.8629\n",
      "Epoch 65/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2543 - accuracy: 0.8751\n",
      "Epoch 00065: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2543 - accuracy: 0.8751\n",
      "Epoch 66/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2708 - accuracy: 0.8682\n",
      "Epoch 00066: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 328ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2696 - accuracy: 0.8682\n",
      "Epoch 67/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2217 - accuracy: 0.8987\n",
      "Epoch 00067: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 340ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2217 - accuracy: 0.8987\n",
      "Epoch 68/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2991 - accuracy: 0.8481\n",
      "Epoch 00068: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 325ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2988 - accuracy: 0.8481\n",
      "Epoch 69/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2298 - accuracy: 0.8885\n",
      "Epoch 00069: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 343ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2298 - accuracy: 0.8885\n",
      "Epoch 70/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2310 - accuracy: 0.8916\n",
      "Epoch 00070: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 329ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2313 - accuracy: 0.8916\n",
      "Epoch 71/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2823 - accuracy: 0.8619\n",
      "Epoch 00071: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 344ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2823 - accuracy: 0.8619\n",
      "Epoch 72/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2574 - accuracy: 0.8747\n",
      "Epoch 00072: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2580 - accuracy: 0.8747\n",
      "Epoch 73/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2323 - accuracy: 0.8849\n",
      "Epoch 00073: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2323 - accuracy: 0.8849\n",
      "Epoch 74/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2933 - accuracy: 0.8532\n",
      "Epoch 00074: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2928 - accuracy: 0.8532\n",
      "Epoch 75/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2929 - accuracy: 0.8574\n",
      "Epoch 00075: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 344ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2929 - accuracy: 0.8574\n",
      "Epoch 76/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2177 - accuracy: 0.8941\n",
      "Epoch 00076: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2185 - accuracy: 0.8941\n",
      "Epoch 77/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2258 - accuracy: 0.8936\n",
      "Epoch 00077: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 341ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2258 - accuracy: 0.8936\n",
      "Epoch 78/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2406 - accuracy: 0.8867\n",
      "Epoch 00078: loss did not improve from 0.21582\n",
      "10/10 [==============================] - 3s 328ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2403 - accuracy: 0.8867\n",
      "Epoch 79/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2018 - accuracy: 0.9059\n",
      "Epoch 00079: loss improved from 0.21582 to 0.20178, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 6s 632ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2018 - accuracy: 0.9059\n",
      "Epoch 80/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2551 - accuracy: 0.8750\n",
      "Epoch 00080: loss did not improve from 0.20178\n",
      "10/10 [==============================] - 3s 337ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2553 - accuracy: 0.8750\n",
      "Epoch 81/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2235 - accuracy: 0.8934\n",
      "Epoch 00081: loss did not improve from 0.20178\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2235 - accuracy: 0.8934\n",
      "Epoch 82/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2083 - accuracy: 0.9014\n",
      "Epoch 00082: loss did not improve from 0.20178\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2086 - accuracy: 0.9014\n",
      "Epoch 83/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2135 - accuracy: 0.8995\n",
      "Epoch 00083: loss did not improve from 0.20178\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2135 - accuracy: 0.8995\n",
      "Epoch 84/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2606 - accuracy: 0.8633\n",
      "Epoch 00084: loss did not improve from 0.20178\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2616 - accuracy: 0.8633\n",
      "Epoch 85/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2866 - accuracy: 0.8495\n",
      "Epoch 00085: loss did not improve from 0.20178\n",
      "10/10 [==============================] - 3s 343ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2866 - accuracy: 0.8495\n",
      "Epoch 86/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2251 - accuracy: 0.8875\n",
      "Epoch 00086: loss did not improve from 0.20178\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2255 - accuracy: 0.8875\n",
      "Epoch 87/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2485 - accuracy: 0.8809\n",
      "Epoch 00087: loss did not improve from 0.20178\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2485 - accuracy: 0.8809\n",
      "Epoch 88/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2293 - accuracy: 0.8960\n",
      "Epoch 00088: loss did not improve from 0.20178\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2282 - accuracy: 0.8960\n",
      "Epoch 89/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2013 - accuracy: 0.9094\n",
      "Epoch 00089: loss improved from 0.20178 to 0.20128, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 6s 640ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2013 - accuracy: 0.9094\n",
      "Epoch 90/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2360 - accuracy: 0.8779\n",
      "Epoch 00090: loss did not improve from 0.20128\n",
      "10/10 [==============================] - 3s 334ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2371 - accuracy: 0.8779\n",
      "Epoch 91/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.1936 - accuracy: 0.9076\n",
      "Epoch 00091: loss improved from 0.20128 to 0.19361, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 5s 467ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1936 - accuracy: 0.9076\n",
      "Epoch 92/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2489 - accuracy: 0.8805\n",
      "Epoch 00092: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2488 - accuracy: 0.8805\n",
      "Epoch 93/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2068 - accuracy: 0.9012\n",
      "Epoch 00093: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 343ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2068 - accuracy: 0.9012\n",
      "Epoch 94/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2212 - accuracy: 0.8894\n",
      "Epoch 00094: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2220 - accuracy: 0.8894\n",
      "Epoch 95/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.1993 - accuracy: 0.9021\n",
      "Epoch 00095: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1993 - accuracy: 0.9021\n",
      "Epoch 96/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2347 - accuracy: 0.8851\n",
      "Epoch 00096: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2355 - accuracy: 0.8851\n",
      "Epoch 97/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2155 - accuracy: 0.8971\n",
      "Epoch 00097: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 343ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2155 - accuracy: 0.8971\n",
      "Epoch 98/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2145 - accuracy: 0.9020\n",
      "Epoch 00098: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2140 - accuracy: 0.9020\n",
      "Epoch 99/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2096 - accuracy: 0.8987\n",
      "Epoch 00099: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2096 - accuracy: 0.8987\n",
      "Epoch 100/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2447 - accuracy: 0.8832\n",
      "Epoch 00100: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 325ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2447 - accuracy: 0.8832\n",
      "Epoch 101/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2203 - accuracy: 0.9015\n",
      "Epoch 00101: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 343ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2203 - accuracy: 0.9015\n",
      "Epoch 102/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2222 - accuracy: 0.8871\n",
      "Epoch 00102: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2232 - accuracy: 0.8871\n",
      "Epoch 103/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2157 - accuracy: 0.8927\n",
      "Epoch 00103: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 344ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2157 - accuracy: 0.8927\n",
      "Epoch 104/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2732 - accuracy: 0.8771\n",
      "Epoch 00104: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 328ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2713 - accuracy: 0.8771\n",
      "Epoch 105/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2909 - accuracy: 0.8546\n",
      "Epoch 00105: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 344ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2909 - accuracy: 0.8546\n",
      "Epoch 106/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2226 - accuracy: 0.8811\n",
      "Epoch 00106: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 328ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2224 - accuracy: 0.8811\n",
      "Epoch 107/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2239 - accuracy: 0.8920\n",
      "Epoch 00107: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2239 - accuracy: 0.8920\n",
      "Epoch 108/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2361 - accuracy: 0.8901\n",
      "Epoch 00108: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 329ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2355 - accuracy: 0.8901\n",
      "Epoch 109/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2234 - accuracy: 0.8907\n",
      "Epoch 00109: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 343ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2234 - accuracy: 0.8907\n",
      "Epoch 110/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2105 - accuracy: 0.9036\n",
      "Epoch 00110: loss did not improve from 0.19361\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2102 - accuracy: 0.9036\n",
      "Epoch 111/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.1882 - accuracy: 0.9102\n",
      "Epoch 00111: loss improved from 0.19361 to 0.18822, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 6s 630ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1882 - accuracy: 0.9102\n",
      "Epoch 112/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2519 - accuracy: 0.8817\n",
      "Epoch 00112: loss did not improve from 0.18822\n",
      "10/10 [==============================] - 3s 333ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2514 - accuracy: 0.8817\n",
      "Epoch 113/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2535 - accuracy: 0.8768\n",
      "Epoch 00113: loss did not improve from 0.18822\n",
      "10/10 [==============================] - 3s 343ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2535 - accuracy: 0.8768\n",
      "Epoch 114/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.1875 - accuracy: 0.9180\n",
      "Epoch 00114: loss improved from 0.18822 to 0.18639, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 4s 460ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1874 - accuracy: 0.9180\n",
      "Epoch 115/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2034 - accuracy: 0.8980\n",
      "Epoch 00115: loss did not improve from 0.18639\n",
      "10/10 [==============================] - 3s 341ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2034 - accuracy: 0.8980\n",
      "Epoch 116/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2170 - accuracy: 0.8951\n",
      "Epoch 00116: loss did not improve from 0.18639\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2172 - accuracy: 0.8951\n",
      "Epoch 117/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.1948 - accuracy: 0.9067\n",
      "Epoch 00117: loss did not improve from 0.18639\n",
      "10/10 [==============================] - 3s 343ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1948 - accuracy: 0.9067\n",
      "Epoch 118/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2124 - accuracy: 0.8898\n",
      "Epoch 00118: loss did not improve from 0.18639\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2128 - accuracy: 0.8898\n",
      "Epoch 119/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.1835 - accuracy: 0.9147\n",
      "Epoch 00119: loss improved from 0.18639 to 0.18354, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 7s 703ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1835 - accuracy: 0.9147\n",
      "Epoch 120/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2214 - accuracy: 0.8874\n",
      "Epoch 00120: loss did not improve from 0.18354\n",
      "10/10 [==============================] - 3s 337ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2223 - accuracy: 0.8874\n",
      "Epoch 121/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2152 - accuracy: 0.8962\n",
      "Epoch 00121: loss did not improve from 0.18354\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2152 - accuracy: 0.8962\n",
      "Epoch 122/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.1959 - accuracy: 0.9058\n",
      "Epoch 00122: loss did not improve from 0.18354\n",
      "10/10 [==============================] - 3s 329ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1956 - accuracy: 0.9058\n",
      "Epoch 123/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.1770 - accuracy: 0.9170\n",
      "Epoch 00123: loss improved from 0.18354 to 0.17696, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 6s 622ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1770 - accuracy: 0.9170\n",
      "Epoch 124/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2476 - accuracy: 0.8760\n",
      "Epoch 00124: loss did not improve from 0.17696\n",
      "10/10 [==============================] - 3s 334ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2476 - accuracy: 0.8760\n",
      "Epoch 125/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2159 - accuracy: 0.9018\n",
      "Epoch 00125: loss did not improve from 0.17696\n",
      "10/10 [==============================] - 3s 341ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2159 - accuracy: 0.9018\n",
      "Epoch 126/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2142 - accuracy: 0.8953\n",
      "Epoch 00126: loss did not improve from 0.17696\n",
      "10/10 [==============================] - 3s 331ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2144 - accuracy: 0.8953\n",
      "Epoch 127/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2316 - accuracy: 0.8883\n",
      "Epoch 00127: loss did not improve from 0.17696\n",
      "10/10 [==============================] - 3s 340ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2316 - accuracy: 0.8883\n",
      "Epoch 128/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2174 - accuracy: 0.9046\n",
      "Epoch 00128: loss did not improve from 0.17696\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2166 - accuracy: 0.9046\n",
      "Epoch 129/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.1838 - accuracy: 0.9160\n",
      "Epoch 00129: loss did not improve from 0.17696\n",
      "10/10 [==============================] - 3s 341ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1838 - accuracy: 0.9160\n",
      "Epoch 130/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2675 - accuracy: 0.8812\n",
      "Epoch 00130: loss did not improve from 0.17696\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2654 - accuracy: 0.8812\n",
      "Epoch 131/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2456 - accuracy: 0.8755\n",
      "Epoch 00131: loss did not improve from 0.17696\n",
      "10/10 [==============================] - 3s 338ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2456 - accuracy: 0.8755\n",
      "Epoch 132/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2258 - accuracy: 0.8992\n",
      "Epoch 00132: loss did not improve from 0.17696\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2241 - accuracy: 0.8992\n",
      "Epoch 133/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2250 - accuracy: 0.8893\n",
      "Epoch 00133: loss did not improve from 0.17696\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2250 - accuracy: 0.8893\n",
      "Epoch 134/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.1560 - accuracy: 0.9263\n",
      "Epoch 00134: loss improved from 0.17696 to 0.16159, saving model to /content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5\n",
      "10/10 [==============================] - 6s 625ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1565 - accuracy: 0.9263\n",
      "Epoch 135/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2046 - accuracy: 0.9069\n",
      "Epoch 00135: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 348ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2046 - accuracy: 0.9069\n",
      "Epoch 136/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2313 - accuracy: 0.8856\n",
      "Epoch 00136: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 328ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2314 - accuracy: 0.8856\n",
      "Epoch 137/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2168 - accuracy: 0.8951\n",
      "Epoch 00137: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 342ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2168 - accuracy: 0.8951\n",
      "Epoch 138/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2298 - accuracy: 0.8980\n",
      "Epoch 00138: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2287 - accuracy: 0.8980\n",
      "Epoch 139/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2000 - accuracy: 0.8996\n",
      "Epoch 00139: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 344ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2000 - accuracy: 0.8996\n",
      "Epoch 140/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2166 - accuracy: 0.8979\n",
      "Epoch 00140: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2161 - accuracy: 0.8979\n",
      "Epoch 141/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.1980 - accuracy: 0.9077\n",
      "Epoch 00141: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 341ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1980 - accuracy: 0.9077\n",
      "Epoch 142/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.1790 - accuracy: 0.9057\n",
      "Epoch 00142: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 330ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1795 - accuracy: 0.9057\n",
      "Epoch 143/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.2191 - accuracy: 0.8941\n",
      "Epoch 00143: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 341ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.2191 - accuracy: 0.8941\n",
      "Epoch 144/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.1759 - accuracy: 0.9169\n",
      "Epoch 00144: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 328ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.1764 - accuracy: 0.9169\n",
      "Epoch 145/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.1943 - accuracy: 0.9018\n",
      "Epoch 00145: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 341ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1943 - accuracy: 0.9018\n",
      "Epoch 146/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2254 - accuracy: 0.8954\n",
      "Epoch 00146: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 331ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2252 - accuracy: 0.8954\n",
      "Epoch 147/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.1982 - accuracy: 0.9053\n",
      "Epoch 00147: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 344ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1982 - accuracy: 0.9053\n",
      "Epoch 148/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2029 - accuracy: 0.9045\n",
      "Epoch 00148: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 326ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2022 - accuracy: 0.9045\n",
      "Epoch 149/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 2.0000 - loss: 0.1997 - accuracy: 0.9017\n",
      "Epoch 00149: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 344ms/step - batch: 4.5000 - size: 2.0000 - loss: 0.1997 - accuracy: 0.9017\n",
      "Epoch 150/150\n",
      "10/10 [==============================] - ETA: 0s - batch: 4.5000 - size: 1.9000 - loss: 0.2045 - accuracy: 0.8972\n",
      "Epoch 00150: loss did not improve from 0.16159\n",
      "10/10 [==============================] - 3s 327ms/step - batch: 4.5000 - size: 1.9000 - loss: 0.2044 - accuracy: 0.8972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f254b72f950>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#保存模型\n",
    "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/2105Dinghu/model/M0831_img_150epochs.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "model.fit_generator(myGene,steps_per_epoch=10,epochs=150,callbacks=[model_checkpoint]) #epoch循环次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "executionInfo": {
     "elapsed": 1119,
     "status": "ok",
     "timestamp": 1560130061462,
     "user": {
      "displayName": "sijin li",
      "photoUrl": "",
      "userId": "04019906230177487561"
     },
     "user_tz": -480
    },
    "id": "6rOag9qOrdSJ",
    "outputId": "281828a7-855f-4f8a-dee2-d94f207887de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 43ms/step\n",
      "(1, 224, 224, 3)\n",
      "(1, 224, 224)\n",
      "(224, 224)\n",
      "[[0.9499326  0.99549216 0.9978957  ... 0.97610724 0.9836714  0.9139656 ]\n",
      " [0.9846599  0.99900335 0.99895877 ... 0.991127   0.98821497 0.95349324]\n",
      " [0.9921744  0.9995446  0.9988524  ... 0.9953227  0.9839264  0.9574317 ]\n",
      " ...\n",
      " [0.9472941  0.9751984  0.96694267 ... 0.99887997 0.99436295 0.9378636 ]\n",
      " [0.9524864  0.9849793  0.98936856 ... 0.9954581  0.9694266  0.9109372 ]\n",
      " [0.83279705 0.9462267  0.9596163  ... 0.97142744 0.88150096 0.66444063]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f49104b64a8>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def testGenerator(test_path,num_image = 30,target_size = (IMG_SIZE,IMG_SIZE),flag_multi_class = False,as_gray = True):\n",
    "    for i in range(num_image):\n",
    "        img = io.imread('/content/data_0521/test/m_1.tif_0_672.tif')#os.path.join(test_path,\"%d.tif\"%i),as_gray = as_gray)\n",
    "        \n",
    "        img = img / 255\n",
    "        img = np.expand_dims(img, axis = 0)\n",
    "        #img = trans.resize(img,target_size)\n",
    "        #img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
    "        #img = np.reshape(img,(1,)+img.shape)\n",
    "        yield img\n",
    "\n",
    "#def labelVisualize(num_class,color_dict,img):\n",
    "#    img = img[:,:,0] if len(img.shape) == 3 else img\n",
    "#    img_out = np.zeros(img.shape + (3,))\n",
    "#    for i in range(num_class):\n",
    "#        img_out[img == i,:] = color_dict[i]\n",
    "#    return img_out / 255\n",
    "\n",
    "def saveResult(save_path,npyfile,flag_multi_class = False,num_class = 2):\n",
    "    for i,item in enumerate(npyfile):\n",
    "#        img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
    "        io.imsave(os.path.join(save_path,\"%d_predict.tif\"%i),item)\n",
    "\n",
    "\n",
    "\n",
    "testGene = testGenerator(\"/content/data_0521/test/\")\n",
    "results = model.predict_generator(testGene,1,verbose=1)\n",
    "print(results.shape)\n",
    "conv_t_1 = results[:,:,:,0]\n",
    "print(conv_t_1.shape)\n",
    "#print(results.shape)\n",
    "#saveResult(\"/content/new_FCN_0422/data\",results)\n",
    "#timg = np.squeeze(results, axis = 3)\n",
    "#print(timg.shape)\n",
    "conv_t_1 = np.squeeze(conv_t_1, axis = 0)\n",
    "print(conv_t_1.shape)\n",
    "print(conv_t_1)\n",
    "plt.imshow(conv_t_1 )\n",
    "#plt.imsave('/content/drive/My Drive/new_FCN_0411/result/m_1.tif_0_672.tif',conv_t_1,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "executionInfo": {
     "elapsed": 2788,
     "status": "ok",
     "timestamp": 1560144227972,
     "user": {
      "displayName": "sijin li",
      "photoUrl": "",
      "userId": "04019906230177487561"
     },
     "user_tz": -480
    },
    "id": "XXxbVt3M7BCj",
    "outputId": "89c8262d-5e35-4026-95a4-f4f82061c08f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "b_1.tif_448_0.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_448_224.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_672_224.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_448_448.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_448_672.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_672_0.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_672_448.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_224_224.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_224_448.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_224_672.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_672_672.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_0_224.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_0_672.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_0_448.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_0_0.tif\n",
      "(1, 224, 224, 3)\n",
      "b_1.tif_224_0.tif\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "#import glob\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def testGenerator(test_path,num_image = 72,target_size = (224,224),flag_multi_class = False,as_gray = True):\n",
    "    for img_name in os.listdir(test_path):\n",
    "        img = io.imread(os.path.join(test_path,\"%s\"%img_name))#,as_gray = as_gray)\n",
    "        img = img / 255\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        #img = trans.resize(img,target_size)\n",
    "        #img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
    "        #img = np.reshape(img,(1,)+img.shape)\n",
    "        #yield img\n",
    "        \n",
    "        results = model.predict(img)\n",
    "        print(results.shape)\n",
    "        saveResult(\"/content/drive/My Drive/new_FCN_0411/result_0610/b/\",results,img_name)\n",
    "        print(img_name)\n",
    "\n",
    "#def labelVisualize(num_class,color_dict,img):\n",
    "#    img = img[:,:,0] if len(img.shape) == 3 else img\n",
    "#    img_out = np.zeros(img.shape + (3,))\n",
    "#    for i in range(num_class):\n",
    "#        img_out[img == i,:] = color_dict[i]\n",
    "#    return img_out / 255\n",
    "\n",
    "def saveResult(save_path,npyfile,img_name,flag_multi_class = False,num_class = 2):\n",
    "    for i,item in enumerate(npyfile):\n",
    "        #print(item)\n",
    "#        img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
    "        io.imsave(os.path.join(save_path,img_name),item)\n",
    "        #plt.imsave('/content/drive/My Drive/FCN/plt_img/t%d.tif'%i,item,cmap='gray')\n",
    "    #timg_1 = np.squeeze(conv_t, axis = 3)\n",
    "    #print(timg_1.shape)\n",
    "    #timg_2 = np.squeeze(timg_1, axis = 0)\n",
    "    #print(timg_2.shape)\n",
    "    #print(timg_2)\n",
    "    #plt.imshow(timg_2)\n",
    "    #plt.imsave('/content/drive/My Drive/FCN/plt_img/t0401_3_test.tif',timg_2,cmap='gray')\n",
    "  \n",
    "testGenerator(\"/content/drive/My Drive/new_FCN_0411/data_0521/test_b/\")\n",
    "#results = model.predict_generator(testGene,72,verbose=1)\n",
    "#saveResult(\"/content/drive/My Drive/new_FCN_0411/result_yulin/\",results,name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "singlechannel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
